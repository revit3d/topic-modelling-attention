{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 100), (100, 40))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "word_size = 40\n",
    "doc_size = 100\n",
    "topic_size = 10\n",
    "\n",
    "data_tokens = np.random.randint(0, vocab_size, size=(doc_size, word_size))\n",
    "\n",
    "data_bow = np.zeros((doc_size, vocab_size))\n",
    "for doc in range(doc_size):\n",
    "    for word in range(word_size):\n",
    "        token = data_tokens[doc][word]\n",
    "        data_bow[doc][token] += 1\n",
    "data_bow /= data_bow.sum(axis=1)[:, None]\n",
    "\n",
    "data_bow.shape, data_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: jax.Array) -> jax.Array:\n",
    "    # take x+ = max(x, 0) element-wise (perform projection on positive simplex)\n",
    "    x = jnp.maximum(x, jnp.zeros_like(x))\n",
    "    # normalize values in non-zero rows to 1 (mapping from the positive simplex to the unit simplex)\n",
    "    norm = x.sum(axis=0)\n",
    "    x = jnp.where(norm > 1e-12, x / norm, jnp.zeros_like(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_norm_vector():\n",
    "    x = np.random.rand(1000) * 100 - 50  # [-50, 50)\n",
    "    y = norm(x)\n",
    "    assert jnp.all(jnp.sign(y) >= 0)\n",
    "    assert jnp.isclose(jnp.sum(y), 1)\n",
    "\n",
    "def test_norm_matrix():\n",
    "    x = np.random.rand(100, 100) * 100 - 50  # [-50, 50)\n",
    "    y = norm(x)\n",
    "    assert jnp.all(jnp.sign(y) >= 0)\n",
    "    assert_allclose(jnp.sum(y, axis=0), 1, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norm_vector()\n",
    "test_norm_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_n_t():\n",
    "    n = word_size * doc_size\n",
    "\n",
    "    n_t = np.zeros(topic_size)\n",
    "    for t in range(topic_size - 1):\n",
    "        n_t[t] = np.random.randint(0, n - np.sum(n_t))\n",
    "    n_t[-1] = n - np.sum(n_t)\n",
    "    assert np.sum(n_t) == n\n",
    "    return n_t\n",
    "\n",
    "\n",
    "def prepare_phi():\n",
    "    phi = np.random.rand(vocab_size, topic_size)\n",
    "    phi = norm(phi)\n",
    "    assert np.sum(phi, axis=0).shape == (topic_size, )\n",
    "    assert_allclose(np.sum(phi, axis=0), 1.0, rtol=1e-6)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = prepare_n_t()\n",
    "phi = prepare_phi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_naive_phi_hatch(phi, n_t):\n",
    "    phi_hatch_naive = np.zeros_like(phi)\n",
    "    for w in range(vocab_size):\n",
    "        for t in range(topic_size):\n",
    "            phi_hatch_naive[w][t] = phi[w][t] * n_t[t]\n",
    "        phi_hatch_naive[w] = norm(phi_hatch_naive[w])\n",
    "    return phi_hatch_naive\n",
    "\n",
    "\n",
    "def calc_fast_phi_hatch(phi, n_t):\n",
    "    return norm(phi.T * n_t[:, None]).T\n",
    "\n",
    "\n",
    "def test_phi_hatch(phi, n_t):\n",
    "    phi_hatch_vec = calc_fast_phi_hatch(phi, n_t)\n",
    "    phi_hatch_naive = calc_naive_phi_hatch(phi, n_t)\n",
    "\n",
    "    assert phi_hatch_vec.shape == phi_hatch_naive.shape\n",
    "    assert_allclose(phi_hatch_vec, phi_hatch_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phi_hatch(phi, n_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_coeff_matrix(ctx_len, seq_len):\n",
    "    gamma = 1 / ctx_len\n",
    "\n",
    "    # construct tril matrix (suffix context)\n",
    "    tril_matrix = np.zeros((seq_len, seq_len))\n",
    "    for i in np.arange(1, ctx_len + 1):\n",
    "        tril_matrix[np.arange(i, seq_len), np.arange(seq_len - i)] = gamma * (1 - gamma) ** i\n",
    "\n",
    "    # contstruct full matrix (self + prefix + suffix context)\n",
    "    full_matrix = np.eye(tril_matrix.shape[0]) * gamma + tril_matrix + tril_matrix.T\n",
    "\n",
    "    # normalize weights and transpose\n",
    "    full_matrix /= full_matrix.sum(axis=0)\n",
    "    full_matrix = full_matrix.T\n",
    "    return jnp.array(full_matrix)\n",
    "\n",
    "\n",
    "def calc_naive_theta(data, phi_hatch, ctx_len):\n",
    "    gamma = 1 / ctx_len\n",
    "    theta = []\n",
    "    context_coeffs = create_context_coeff_matrix(ctx_len=ctx_len, seq_len=word_size)\n",
    "    for d in range(doc_size):\n",
    "        for w in range(word_size):\n",
    "            left_context_vec = np.zeros(topic_size)\n",
    "            right_context_vec = np.zeros(topic_size)\n",
    "            for i in range(1, ctx_len + 1):\n",
    "                if w + i < word_size:\n",
    "                    left_context_vec += context_coeffs[w][w + i] * phi_hatch[data[d][w + i]]\n",
    "                if w - i >= 0:\n",
    "                    right_context_vec += context_coeffs[w][w - i] * phi_hatch[data[d][w - i]]\n",
    "            theta.append(left_context_vec + right_context_vec + context_coeffs[w][w] * phi_hatch[data[d][w]])\n",
    "    return np.array(theta)\n",
    "\n",
    "\n",
    "def calc_fast_theta(data, phi_hatch, ctx_len):\n",
    "    context_coeffs = create_context_coeff_matrix(ctx_len=ctx_len, seq_len=word_size)\n",
    "    data_emb = jnp.take_along_axis(phi_hatch[None, ...], indices=data[..., None], axis=1)  # (D, W_d, T)\n",
    "    theta_new = jnp.sum(\n",
    "        data_emb[:, None, :, :] * context_coeffs[None, :, :, None],\n",
    "        axis=2,\n",
    "    )  # (D, W_d, T)\n",
    "    theta_new = theta_new.reshape(-1, theta_new.shape[-1])  # (I, T)\n",
    "    return theta_new\n",
    "\n",
    "\n",
    "def test_theta(data, phi_hatch, ctx_len=8):\n",
    "    theta_naive = calc_naive_theta(data, phi_hatch, ctx_len)\n",
    "    theta_fast = calc_fast_theta(data, phi_hatch, ctx_len)\n",
    "    assert theta_naive.shape == theta_fast.shape\n",
    "    assert_allclose(theta_fast, theta_naive, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_hatch = calc_fast_phi_hatch(phi, n_t)\n",
    "test_theta(data_tokens, phi_hatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fast_p_ti(phi, data, theta_new):\n",
    "    data_emb = jnp.take_along_axis(phi[None, ...], indices=data[..., None], axis=1)  # (D, W_d, T)\n",
    "    p_ti = data_emb.reshape(-1, data_emb.shape[-1])  # (I, T)\n",
    "    p_ti = norm((p_ti * theta_new).T).T  # (I, T)\n",
    "    return p_ti\n",
    "\n",
    "\n",
    "def calc_naive_p_ti(phi, data, theta_new):\n",
    "    p_ti_naive = np.zeros((word_size * doc_size, topic_size))\n",
    "    for d in range(doc_size):\n",
    "        for w in range(word_size):\n",
    "            i = d * word_size + w\n",
    "            token = data[d][w]\n",
    "            for t in range(topic_size):\n",
    "                p_ti_naive[i][t] = phi[token][t] * theta_new[i][t]\n",
    "            p_ti_naive[i] = norm(p_ti_naive[i])\n",
    "    return p_ti_naive\n",
    "\n",
    "\n",
    "def test_p_ti(phi, data, theta_new):\n",
    "    p_ti_fast = calc_fast_p_ti(phi, data, theta_new)\n",
    "    p_ti_naive = calc_naive_p_ti(phi, data, theta_new)\n",
    "\n",
    "    assert p_ti_fast.shape == p_ti_naive.shape\n",
    "    assert_allclose(p_ti_fast, p_ti_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_new = calc_fast_theta(data_tokens, phi_hatch, ctx_len=8)\n",
    "test_p_ti(phi, data_tokens, theta_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_naive_n_t(p_ti):\n",
    "    n_t = np.zeros(topic_size)\n",
    "    for i in range(word_size * doc_size):\n",
    "        for t in range(topic_size):\n",
    "            n_t[t] += p_ti[i][t]\n",
    "    return n_t\n",
    "\n",
    "\n",
    "def calc_fast_n_t(p_ti):\n",
    "    return jnp.sum(p_ti, axis=0)  # (T, )\n",
    "\n",
    "\n",
    "def test_n_t(p_ti):\n",
    "    n_t_naive = calc_naive_n_t(p_ti)\n",
    "    n_t_fast = calc_fast_n_t(p_ti)\n",
    "\n",
    "    assert n_t_naive.shape == n_t_fast.shape\n",
    "    assert_allclose(n_t_fast, n_t_naive, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ti = calc_fast_p_ti(phi, data_tokens, theta_new)\n",
    "test_n_t(p_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_naive_phi(data, p_ti):\n",
    "    phi_new = np.zeros((topic_size, vocab_size))\n",
    "    for t in range(topic_size):\n",
    "        for word in range(vocab_size):\n",
    "            for d in range(doc_size):\n",
    "                for w in range(word_size):\n",
    "                    i = d * word_size + w\n",
    "                    token = data[d][w]\n",
    "                    phi_new[t][word] += (token == word) * p_ti[i][t]\n",
    "        phi_new[t] = norm(phi_new[t])\n",
    "    return phi_new.T\n",
    "\n",
    "\n",
    "def calc_fast_phi(data, p_ti):\n",
    "    indices = data.flatten()  # (I, )\n",
    "    phi_new = jnp.add.at(jnp.zeros((vocab_size, topic_size)), indices, p_ti, inplace=False)  # (W, T)\n",
    "    phi_new = norm(phi_new)  # (W, T)\n",
    "    return phi_new\n",
    "\n",
    "\n",
    "def test_phi(data, p_ti):\n",
    "    phi_naive = calc_naive_phi(data, p_ti)\n",
    "    phi_fast = calc_fast_phi(data, p_ti)\n",
    "\n",
    "    assert phi_naive.shape == phi_fast.shape\n",
    "    assert_allclose(phi_fast, phi_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:36<00:00, 27.66s/it]\n"
     ]
    }
   ],
   "source": [
    "test_phi(data_tokens, p_ti)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-modelling-attention-srLx0cG6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
