{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_prepocessing import ArticlesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>From: rdell@cbnewsf.cb.att.com (richard.b.dell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>From: chriss@netcom.com (Chris Silvester)\\nSub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      From: Mamatha Devineni Ratnam <mr47+@andrew.cm...\n",
       "1      From: mblawson@midway.ecn.uoknor.edu (Matthew ...\n",
       "2      From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...\n",
       "3      From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...\n",
       "4      From: Alexander Samuel McDiarmid <am2o+@andrew...\n",
       "...                                                  ...\n",
       "18841  From: jim.zisfein@factory.com (Jim Zisfein) \\n...\n",
       "18842  From: rdell@cbnewsf.cb.att.com (richard.b.dell...\n",
       "18843  From: westes@netcom.com (Will Estes)\\nSubject:...\n",
       "18844  From: steve@hcrlgw (Steven Collins)\\nSubject: ...\n",
       "18845  From: chriss@netcom.com (Chris Silvester)\\nSub...\n",
       "\n",
       "[18846 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_20newsgroups(data_home='./data/', subset='all').data\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>TV's future down the phone line\\n\\nInternet TV...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>Cebit fever takes over Hanover\\n\\nThousands of...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
       "...                                                 ...       ...\n",
       "2197  TV's future down the phone line\\n\\nInternet TV...      tech\n",
       "2198  Cebit fever takes over Hanover\\n\\nThousands of...      tech\n",
       "2199  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
       "2200  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
       "2201  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
       "\n",
       "[2202 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/bbc_text_cls.csv')\n",
    "maxlen = np.quantile(df.text.apply(len), q=0.99)\n",
    "print(len(df))\n",
    "df = df[df.text.apply(len) < maxlen].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_coeff_matrix(ctx_len, seq_len):\n",
    "    gamma = 1 / ctx_len\n",
    "\n",
    "    # construct tril matrix (suffix context)\n",
    "    tril_matrix = np.zeros((seq_len, seq_len))\n",
    "    for i in np.arange(1, ctx_len + 1):\n",
    "        tril_matrix[np.arange(i, seq_len), np.arange(seq_len - i)] = gamma * (1 - gamma) ** i\n",
    "\n",
    "    # contstruct full matrix (self + prefix + suffix context)\n",
    "    full_matrix = tril_matrix + tril_matrix.T + np.eye(tril_matrix.shape[0]) * gamma\n",
    "\n",
    "    # normalize weights and transpose\n",
    "    full_matrix /= full_matrix.sum(axis=0)\n",
    "    full_matrix = full_matrix.T\n",
    "    return jnp.array(full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = create_context_coeff_matrix(ctx_len=8, seq_len=20)\n",
    "sns.heatmap(cf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[2, 3], [4, 5]])\n",
    "\n",
    "b = np.array([0, 0, 1, 0])\n",
    "\n",
    "np.take_along_axis(a, b[:, None], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = create_context_coeff_matrix(ctx_len=2, seq_len=4)\n",
    "sns.heatmap(cf, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3],\n",
       "        [4, 4]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 1], [2, 2], [3, 3], [4, 4]])[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4, 2),\n",
       " Array([[[1.5714287, 1.5714287],\n",
       "         [2.2222223, 2.2222223],\n",
       "         [2.777778 , 2.777778 ],\n",
       "         [3.4285717, 3.4285717]],\n",
       " \n",
       "        [[2.142857 , 3.1428573],\n",
       "         [3.5555556, 4.555556 ],\n",
       "         [4.7777777, 5.777778 ],\n",
       "         [6.4285717, 7.428572 ]]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = jnp.array([\n",
    "    [[1, 1], [2, 2], [3, 3], [4, 4]],\n",
    "    # 0.57 * [1, 1] + 0.29 * [2, 2] + 0.14 * [3, 3] + 0    * [4, 4] = [1.57, 1.57]\n",
    "    # 0.22 * [1, 1] + 0.44 * [2, 2] + 0.22 * [3, 3] + 0.11 * [4, 4] = [2.2, 2.2]\n",
    "    # 0.11 * [1, 1] + 0.22 * [2, 2] + 0.44 * [3, 3] + 0.22 * [4, 4] = [2.75, 2.75]\n",
    "    # 0    * [1, 1] + 0.14 * [2, 2] + 0.29 * [3, 3] + 0.57 * [4, 4] = [3.43, 3.43]\n",
    "    [[1, 2], [3, 4], [5, 6], [8, 9]],\n",
    "])  # (D, W, T)\n",
    "# (W, W)\n",
    "\n",
    "# res = np.einsum('hwk,wj->hwk', tensor, cf)\n",
    "# res = np.dot(tensor, cf[None, ...])\n",
    "# res = fftconvolve(tensor[:, None, :, :], cf[::-1][None, :, :, None], mode='valid', axes=2)\n",
    "res = jnp.sum(tensor[:, None, :, :] * cf[None, :, :, None], axis=2)\n",
    "# res = res.sum(axis=2)\n",
    "res.shape, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.5714287, 1.5714287],\n",
       "       [2.2222223, 2.2222223],\n",
       "       [2.777778 , 2.777778 ],\n",
       "       [3.4285717, 3.4285717],\n",
       "       [2.142857 , 3.1428573],\n",
       "       [3.5555556, 4.555556 ],\n",
       "       [4.7777777, 5.777778 ],\n",
       "       [6.4285717, 7.428572 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.reshape(-1, res.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 500, 10) (500, 500)\n",
      "Testing base function:\n",
      "2.81 s ± 138 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Testing jit-compilation on base function:\n",
      "The slowest run took 4.50 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "2.31 s ± 1 s per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Testing inline function:\n",
      "11.9 s ± 321 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(42)\n",
    "tensor = jax.random.normal(\n",
    "    key=key,\n",
    "    shape=(2000, 500, 10),\n",
    ")  # (D, W, T)\n",
    "cf = create_context_coeff_matrix(ctx_len=5, seq_len=500)\n",
    "print(tensor.shape, cf.shape)\n",
    "\n",
    "def foo(x, y):\n",
    "    return jnp.sum(x[:, None, :, :] * y[None, :, :, None], axis=2)\n",
    "\n",
    "print('Testing base function:')\n",
    "%timeit foo(tensor, cf)\n",
    "# print('Testing vectorized function:')\n",
    "# vf = jax.vmap(foo)\n",
    "# %timeit vf(tensor[:, None, :, :], cf[None, :, :, None])\n",
    "print('Testing jit-compilation on base function:')\n",
    "jf = jax.jit(foo)\n",
    "%timeit jf(tensor, cf)\n",
    "# print('Testing jit-compilation on vectorized function:')\n",
    "# jvf = jax.jit(vf)\n",
    "# %timeit jvf(tensor, cf)\n",
    "print('Testing inline function:')\n",
    "%timeit jnp.sum(tensor[:, None, :, :] * cf[None, :, :, None], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5, 0. , 1. , 0.4],\n",
       "       [0.5, 1. , 0. , 0.6]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def _norm(x: jax.Array):\n",
    "    # take x+ = max(x, 0) element-wise (perform projection on positive simplex)\n",
    "    x = jnp.maximum(x, jnp.zeros_like(x))\n",
    "    # normalize values in non-zero rows to 1 (mapping from the positive simplex to the unit simplex)\n",
    "    norm = x.sum(axis=0)\n",
    "    x = jnp.where(norm > 1e-12, x / norm, jnp.zeros_like(x))\n",
    "    return x\n",
    "\n",
    "_norm(\n",
    "    jnp.array([\n",
    "        [1, -1, 1, 2],\n",
    "        [1, 3, -4, 3],\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3  0.9]\n",
      " [ 4.   8. ]\n",
      " [10.   4. ]\n",
      " [ 1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "indices = np.array([0, 0, 0, 1, 2, 1, 3, 2])\n",
    "values = np.array([[0.1, 0.3], [2, 4], [5, 2], [1, 0]])\n",
    "\n",
    "# Create a one-hot encoding of the indices\n",
    "one_hot_indices = np.eye(indices.max() + 1)[indices]\n",
    "\n",
    "# Multiply the one-hot encoding by the values\n",
    "multiplied_values = values * one_hot_indices.sum(axis=0)[:, None]\n",
    "\n",
    "print(multiplied_values)  # Output: [0.2, 4, 0.1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextTopicModelDebug():\n",
    "    \"\"\"\n",
    "    Topic model which uses local context of words\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ctx_len: int,\n",
    "            max_len: int,\n",
    "            vocab_size: int,\n",
    "            n_topics: int = 10,\n",
    "            reg_list: list = None,\n",
    "            eps: float = 1e-12,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ctx_len: one-sided context size\n",
    "            max_len: max length of a document, W_d\n",
    "            vocab_size: corpus vocabulary size, W\n",
    "            n_topics: number of topics, T\n",
    "            regularizations: list of regularizations (see `add_regularization` method)\n",
    "            eps: parameter set for balance between numerical stability and precision\n",
    "\n",
    "        Note:\n",
    "            - Total context of a word on `i`-th index is ctx_len words to the left,\\\\\n",
    "            `ctx_len` words to the right, and the word itself\n",
    "            - All documents should be padded to `max_len` length\n",
    "        \"\"\"\n",
    "        self.ctx_len = ctx_len\n",
    "        self.seq_len = max_len\n",
    "        self.n_topics = n_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        self._eps = eps\n",
    "\n",
    "        self._context_coeffs = self._create_context_coeff_matrix()\n",
    "\n",
    "        self._regularizations = dict()\n",
    "        if reg_list is not None:\n",
    "            for reg in reg_list:\n",
    "                self.add_regularization(reg)\n",
    "\n",
    "    def _norm(self, x: jax.Array) -> jax.Array:\n",
    "        assert jnp.any(~jnp.isnan(x)), jnp.sum(x)\n",
    "        # take x+ = max(x, 0) element-wise (perform projection on positive simplex)\n",
    "        x = jnp.maximum(x, jnp.zeros_like(x))\n",
    "        # normalize values in non-zero rows to 1 (mapping from the positive simplex to the unit simplex)\n",
    "        norm = x.sum(axis=0)\n",
    "        x = jnp.where(norm > self._eps, x / norm, jnp.zeros_like(x))\n",
    "        return x\n",
    "\n",
    "    def _create_context_coeff_matrix(self) -> jax.Array:\n",
    "        gamma = 1 / self.ctx_len\n",
    "\n",
    "        # construct tril matrix (suffix context)\n",
    "        tril_matrix = np.zeros((self.seq_len, self.seq_len))\n",
    "        for i in np.arange(1, self.ctx_len + 1):\n",
    "            tril_matrix[np.arange(i, self.seq_len), np.arange(self.seq_len - i)] = gamma * (1 - gamma) ** i\n",
    "\n",
    "        # contstruct full matrix (self + prefix + suffix context)\n",
    "        full_matrix = tril_matrix + tril_matrix.T # + np.eye(tril_matrix.shape[0]) * gamma \n",
    "\n",
    "        # normalize weights and transpose\n",
    "        full_matrix /= full_matrix.sum(axis=0)\n",
    "        full_matrix = full_matrix.T\n",
    "        return jnp.array(full_matrix)\n",
    "\n",
    "    def add_regularization(self, reg, tag: str = None):\n",
    "        \"\"\"\n",
    "        Add `reg` regularization to the model with `tag` identifier \\\\\n",
    "        Note:\n",
    "        - `reg` has to be a child of base `Regularization` class\n",
    "        - `tag` will use the name of the class by default\n",
    "        \"\"\"\n",
    "        if tag is None:\n",
    "            tag = reg.__name__\n",
    "        if not isinstance(reg, Regularization):\n",
    "            raise TypeError(f'Regularization [{tag}] has to be a subclass of Regularization class')\n",
    "\n",
    "        try:\n",
    "            self._regularizations[tag] = jax.grad(reg)\n",
    "        except Exception:\n",
    "            raise\n",
    "\n",
    "    def _compose_regularizations(self):\n",
    "        regs = self._regularizations.values()\n",
    "        sum_reg = lambda x: sum([1.0, ] + [reg(x) for reg in regs])\n",
    "        return jax.jit(jax.grad(sum_reg))\n",
    "\n",
    "    def fit(self, data: jax.Array, max_iter: int = 1000, tol: float = 1e-3, seed: int = 0, test_data=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: matrix of shape (D, W_d), containing tokenized words of each document\n",
    "            max_iter: max number of iterations\n",
    "            tol: early stopping threshold\n",
    "            seed: random seed\n",
    "        \"\"\"\n",
    "        key = jax.random.key(seed)\n",
    "        self.phi = jax.random.uniform(\n",
    "            key=key,\n",
    "            shape=(self.vocab_size, self.n_topics),\n",
    "        )  # (W, T)\n",
    "        self.n_t = jnp.full(\n",
    "            shape=(self.n_topics, ),\n",
    "            fill_value=jnp.sum(data, dtype=jnp.float32) / self.n_topics,\n",
    "        )  # (T, )\n",
    "        grad_regularization = self._compose_regularizations()\n",
    "\n",
    "        self.phi = self._norm(self.phi)\n",
    "        t_cur = time.time()\n",
    "        for it in range(max_iter):\n",
    "            # Calculate phi' (words -> topics) matrix (phi with old p_{ti})\n",
    "            print(np.array(self.phi).shape, np.array(self.n_t).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_hatch = self._norm(self.phi.T * self.n_t[:, None]).T  # (W, T)\n",
    "            print(np.array(phi_hatch).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Create theta (documents -> topics) matrix\n",
    "            # We can interpret phi as a bunch of embeddings for words,\n",
    "            # thus creating tensor of embeddings of words in documents\n",
    "            # and calculating context (document embedding), based on this\n",
    "            data_emb = jnp.take_along_axis(phi_hatch[None, ...], indices=data[..., None], axis=1)  # (D, W_d, T)\n",
    "            print(np.array(data_emb)[:, None, :, :].shape, self._context_coeffs[::-1][None, :, :, None].shape, f'{time.time() - t_cur:.01f}')\n",
    "            theta_new = jnp.sum(\n",
    "                data_emb[:, None, :, :] * self._context_coeffs[None, :, :, None],\n",
    "                axis=2,\n",
    "            )  # (D, W_d, T)\n",
    "            print(np.array(theta_new).shape, f'{time.time() - t_cur:.01f}')\n",
    "            # Now we see each context window as a new document, I - number of context documents\n",
    "            theta_new = theta_new.reshape(-1, theta_new.shape[-1])  # (I, T)\n",
    "\n",
    "            # Update p_{ti} - topic probability distribution for i-th context\n",
    "            data_emb = jnp.take_along_axis(self.phi[None, ...], indices=data[..., None], axis=1)  # (D, W_d, T)\n",
    "            print(np.array(data_emb).shape, f'{time.time() - t_cur:.01f}')\n",
    "            p_ti = data_emb.reshape(-1, data_emb.shape[-1])  # (I, T)\n",
    "            p_ti = self._norm((p_ti * theta_new).T).T  # (I, T)\n",
    "            print(np.array(p_ti).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Update n_{t} - topic probability distribution\n",
    "            self.n_t = jnp.sum(p_ti, axis=0)  # (T, )\n",
    "            print(np.array(self.n_t).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Update phi (words -> topics) matrix (phi with new p_{ti})\n",
    "            indices = data.flatten()  # (I, )\n",
    "            phi_new = jnp.add.at(jnp.zeros_like(self.phi), indices, p_ti, inplace=False)  # (W, T)\n",
    "            print(np.array(phi_new).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_new += self.phi * grad_regularization(self.phi)  # (W, T)\n",
    "            print(np.array(phi_new).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_new = self._norm(phi_new)  # (W, T)\n",
    "\n",
    "            diff_norm = jnp.linalg.norm(phi_new - self.phi)\n",
    "            theta_doc = theta_new.reshape(data.shape + (-1, )).sum(axis=1)\n",
    "            res_diff_norm = jnp.exp(-jnp.sum(test_data * jnp.log(theta_doc @ phi_new.T + self._eps)) / (data.shape[0] * data.shape[1]))\n",
    "            print(f'Iteration [{it}/{max_iter}], update diff norm: {diff_norm:.04f}, perplexity: {res_diff_norm:.04f}')\n",
    "            self.phi = phi_new\n",
    "            if diff_norm < tol:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 100\n",
    "dataset = ArticlesDataset(df.text.tolist(), maxlen=maxlen)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diyakovilya/Library/Caches/pypoetry/virtualenvs/topic-modelling-attention-srLx0cG6-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/diyakovilya/Library/Caches/pypoetry/virtualenvs/topic-modelling-attention-srLx0cG6-py3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18846, 107673)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(\n",
    "    preprocessor=lambda x: ' '.join(ArticlesDataset.preprocess_text(x)),\n",
    "    tokenizer=word_tokenize,\n",
    "    vocabulary=dataset.vocab,\n",
    ").fit_transform(df.text.to_list())\n",
    "bow = jnp.array(bow.todense())\n",
    "# bow /= bow.sum(axis=1)[:, None]\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107673, 10) (10,) 0.0\n",
      "(107673, 10) 0.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 0.0\n",
      "(18846, 100, 10) 0.7\n",
      "(18846, 100, 10) 0.7\n",
      "(1884600, 10) 0.8\n",
      "(10,) 0.8\n",
      "(107673, 10) 0.8\n",
      "(107673, 10) 0.9\n",
      "Iteration [0/1000], update diff norm: 0.5189, perplexity: 5962.7715\n",
      "(107673, 10) (10,) 4.7\n",
      "(107673, 10) 4.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 4.7\n",
      "(18846, 100, 10) 5.4\n",
      "(18846, 100, 10) 5.4\n",
      "(1884600, 10) 5.5\n",
      "(10,) 5.5\n",
      "(107673, 10) 5.6\n",
      "(107673, 10) 5.6\n",
      "Iteration [1/1000], update diff norm: 0.1516, perplexity: 5956.8662\n",
      "(107673, 10) (10,) 9.8\n",
      "(107673, 10) 9.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 9.8\n",
      "(18846, 100, 10) 10.5\n",
      "(18846, 100, 10) 10.6\n",
      "(1884600, 10) 10.6\n",
      "(10,) 10.6\n",
      "(107673, 10) 10.7\n",
      "(107673, 10) 10.7\n",
      "Iteration [2/1000], update diff norm: 0.1706, perplexity: 5861.3770\n",
      "(107673, 10) (10,) 16.1\n",
      "(107673, 10) 16.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 16.1\n",
      "(18846, 100, 10) 16.8\n",
      "(18846, 100, 10) 16.8\n",
      "(1884600, 10) 16.9\n",
      "(10,) 16.9\n",
      "(107673, 10) 17.0\n",
      "(107673, 10) 17.0\n",
      "Iteration [3/1000], update diff norm: 0.1983, perplexity: 5713.8379\n",
      "(107673, 10) (10,) 20.0\n",
      "(107673, 10) 20.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 20.0\n",
      "(18846, 100, 10) 20.7\n",
      "(18846, 100, 10) 20.7\n",
      "(1884600, 10) 20.8\n",
      "(10,) 20.8\n",
      "(107673, 10) 20.8\n",
      "(107673, 10) 20.8\n",
      "Iteration [4/1000], update diff norm: 0.1989, perplexity: 5461.9092\n",
      "(107673, 10) (10,) 24.0\n",
      "(107673, 10) 24.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 24.0\n",
      "(18846, 100, 10) 24.7\n",
      "(18846, 100, 10) 24.8\n",
      "(1884600, 10) 24.8\n",
      "(10,) 24.8\n",
      "(107673, 10) 24.9\n",
      "(107673, 10) 24.9\n",
      "Iteration [5/1000], update diff norm: 0.1283, perplexity: 5144.2373\n",
      "(107673, 10) (10,) 29.1\n",
      "(107673, 10) 29.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 29.1\n",
      "(18846, 100, 10) 29.8\n",
      "(18846, 100, 10) 29.8\n",
      "(1884600, 10) 29.9\n",
      "(10,) 29.9\n",
      "(107673, 10) 30.0\n",
      "(107673, 10) 30.0\n",
      "Iteration [6/1000], update diff norm: 0.0578, perplexity: 4856.9404\n",
      "(107673, 10) (10,) 33.6\n",
      "(107673, 10) 33.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 33.6\n",
      "(18846, 100, 10) 34.3\n",
      "(18846, 100, 10) 34.3\n",
      "(1884600, 10) 34.4\n",
      "(10,) 34.4\n",
      "(107673, 10) 34.5\n",
      "(107673, 10) 34.5\n",
      "Iteration [7/1000], update diff norm: 0.1059, perplexity: 4646.3540\n",
      "(107673, 10) (10,) 37.6\n",
      "(107673, 10) 37.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 37.6\n",
      "(18846, 100, 10) 38.3\n",
      "(18846, 100, 10) 38.3\n",
      "(1884600, 10) 38.4\n",
      "(10,) 38.4\n",
      "(107673, 10) 38.5\n",
      "(107673, 10) 38.5\n",
      "Iteration [8/1000], update diff norm: 0.1973, perplexity: 4473.2881\n",
      "(107673, 10) (10,) 44.2\n",
      "(107673, 10) 44.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 44.2\n",
      "(18846, 100, 10) 44.9\n",
      "(18846, 100, 10) 45.0\n",
      "(1884600, 10) 45.0\n",
      "(10,) 45.0\n",
      "(107673, 10) 45.1\n",
      "(107673, 10) 45.1\n",
      "Iteration [9/1000], update diff norm: 0.2230, perplexity: 4321.8711\n",
      "(107673, 10) (10,) 49.1\n",
      "(107673, 10) 49.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 49.1\n",
      "(18846, 100, 10) 49.8\n",
      "(18846, 100, 10) 49.8\n",
      "(1884600, 10) 49.8\n",
      "(10,) 49.8\n",
      "(107673, 10) 49.9\n",
      "(107673, 10) 49.9\n",
      "Iteration [10/1000], update diff norm: 0.1630, perplexity: 4198.2676\n",
      "(107673, 10) (10,) 54.2\n",
      "(107673, 10) 54.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 54.3\n",
      "(18846, 100, 10) 55.0\n",
      "(18846, 100, 10) 55.0\n",
      "(1884600, 10) 55.1\n",
      "(10,) 55.1\n",
      "(107673, 10) 55.2\n",
      "(107673, 10) 55.2\n",
      "Iteration [11/1000], update diff norm: 0.0619, perplexity: 4100.1426\n",
      "(107673, 10) (10,) 58.9\n",
      "(107673, 10) 58.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 58.9\n",
      "(18846, 100, 10) 59.6\n",
      "(18846, 100, 10) 59.6\n",
      "(1884600, 10) 59.6\n",
      "(10,) 59.6\n",
      "(107673, 10) 59.7\n",
      "(107673, 10) 59.7\n",
      "Iteration [12/1000], update diff norm: 0.0209, perplexity: 4018.4429\n",
      "(107673, 10) (10,) 64.4\n",
      "(107673, 10) 64.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 64.4\n",
      "(18846, 100, 10) 65.2\n",
      "(18846, 100, 10) 65.2\n",
      "(1884600, 10) 65.3\n",
      "(10,) 65.3\n",
      "(107673, 10) 65.4\n",
      "(107673, 10) 65.4\n",
      "Iteration [13/1000], update diff norm: 0.0166, perplexity: 3947.2939\n",
      "(107673, 10) (10,) 70.1\n",
      "(107673, 10) 70.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 70.1\n",
      "(18846, 100, 10) 70.8\n",
      "(18846, 100, 10) 70.9\n",
      "(1884600, 10) 70.9\n",
      "(10,) 70.9\n",
      "(107673, 10) 71.0\n",
      "(107673, 10) 71.0\n",
      "Iteration [14/1000], update diff norm: 0.0147, perplexity: 3883.4702\n",
      "(107673, 10) (10,) 75.1\n",
      "(107673, 10) 75.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 75.1\n",
      "(18846, 100, 10) 75.9\n",
      "(18846, 100, 10) 75.9\n",
      "(1884600, 10) 76.0\n",
      "(10,) 76.0\n",
      "(107673, 10) 76.1\n",
      "(107673, 10) 76.1\n",
      "Iteration [15/1000], update diff norm: 0.0133, perplexity: 3824.3894\n",
      "(107673, 10) (10,) 80.8\n",
      "(107673, 10) 80.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 80.8\n",
      "(18846, 100, 10) 81.5\n",
      "(18846, 100, 10) 81.5\n",
      "(1884600, 10) 81.6\n",
      "(10,) 81.6\n",
      "(107673, 10) 81.6\n",
      "(107673, 10) 81.6\n",
      "Iteration [16/1000], update diff norm: 0.0119, perplexity: 3768.7727\n",
      "(107673, 10) (10,) 86.5\n",
      "(107673, 10) 86.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 86.5\n",
      "(18846, 100, 10) 87.2\n",
      "(18846, 100, 10) 87.2\n",
      "(1884600, 10) 87.3\n",
      "(10,) 87.3\n",
      "(107673, 10) 87.3\n",
      "(107673, 10) 87.3\n",
      "Iteration [17/1000], update diff norm: 0.0103, perplexity: 3716.5515\n",
      "(107673, 10) (10,) 91.1\n",
      "(107673, 10) 91.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 91.1\n",
      "(18846, 100, 10) 91.8\n",
      "(18846, 100, 10) 91.8\n",
      "(1884600, 10) 91.9\n",
      "(10,) 91.9\n",
      "(107673, 10) 92.0\n",
      "(107673, 10) 92.0\n",
      "Iteration [18/1000], update diff norm: 0.0091, perplexity: 3668.1938\n",
      "(107673, 10) (10,) 96.6\n",
      "(107673, 10) 96.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 96.6\n",
      "(18846, 100, 10) 97.3\n",
      "(18846, 100, 10) 97.4\n",
      "(1884600, 10) 97.4\n",
      "(10,) 97.4\n",
      "(107673, 10) 97.5\n",
      "(107673, 10) 97.5\n",
      "Iteration [19/1000], update diff norm: 0.0082, perplexity: 3623.6780\n",
      "(107673, 10) (10,) 102.2\n",
      "(107673, 10) 102.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 102.2\n",
      "(18846, 100, 10) 102.9\n",
      "(18846, 100, 10) 103.0\n",
      "(1884600, 10) 103.0\n",
      "(10,) 103.0\n",
      "(107673, 10) 103.1\n",
      "(107673, 10) 103.1\n",
      "Iteration [20/1000], update diff norm: 0.0076, perplexity: 3582.5984\n",
      "(107673, 10) (10,) 107.1\n",
      "(107673, 10) 107.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 107.2\n",
      "(18846, 100, 10) 107.8\n",
      "(18846, 100, 10) 107.8\n",
      "(1884600, 10) 107.9\n",
      "(10,) 107.9\n",
      "(107673, 10) 108.0\n",
      "(107673, 10) 108.0\n",
      "Iteration [21/1000], update diff norm: 0.0070, perplexity: 3544.7080\n",
      "(107673, 10) (10,) 112.1\n",
      "(107673, 10) 112.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 112.1\n",
      "(18846, 100, 10) 112.8\n",
      "(18846, 100, 10) 112.8\n",
      "(1884600, 10) 112.9\n",
      "(10,) 112.9\n",
      "(107673, 10) 113.0\n",
      "(107673, 10) 113.0\n",
      "Iteration [22/1000], update diff norm: 0.0065, perplexity: 3509.8418\n",
      "(107673, 10) (10,) 117.0\n",
      "(107673, 10) 117.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 117.1\n",
      "(18846, 100, 10) 117.7\n",
      "(18846, 100, 10) 117.8\n",
      "(1884600, 10) 117.8\n",
      "(10,) 117.8\n",
      "(107673, 10) 117.9\n",
      "(107673, 10) 117.9\n",
      "Iteration [23/1000], update diff norm: 0.0060, perplexity: 3477.8716\n",
      "(107673, 10) (10,) 122.0\n",
      "(107673, 10) 122.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 122.0\n",
      "(18846, 100, 10) 122.7\n",
      "(18846, 100, 10) 122.8\n",
      "(1884600, 10) 122.8\n",
      "(10,) 122.8\n",
      "(107673, 10) 122.9\n",
      "(107673, 10) 122.9\n",
      "Iteration [24/1000], update diff norm: 0.0057, perplexity: 3448.6516\n",
      "(107673, 10) (10,) 127.3\n",
      "(107673, 10) 127.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 127.4\n",
      "(18846, 100, 10) 128.0\n",
      "(18846, 100, 10) 128.1\n",
      "(1884600, 10) 128.2\n",
      "(10,) 128.2\n",
      "(107673, 10) 128.3\n",
      "(107673, 10) 128.3\n",
      "Iteration [25/1000], update diff norm: 0.0053, perplexity: 3422.1829\n",
      "(107673, 10) (10,) 133.4\n",
      "(107673, 10) 133.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 133.5\n",
      "(18846, 100, 10) 134.1\n",
      "(18846, 100, 10) 134.2\n",
      "(1884600, 10) 134.2\n",
      "(10,) 134.2\n",
      "(107673, 10) 134.3\n",
      "(107673, 10) 134.3\n",
      "Iteration [26/1000], update diff norm: 0.0050, perplexity: 3398.3955\n",
      "(107673, 10) (10,) 138.2\n",
      "(107673, 10) 138.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 138.3\n",
      "(18846, 100, 10) 138.9\n",
      "(18846, 100, 10) 139.0\n",
      "(1884600, 10) 139.0\n",
      "(10,) 139.0\n",
      "(107673, 10) 139.1\n",
      "(107673, 10) 139.1\n",
      "Iteration [27/1000], update diff norm: 0.0047, perplexity: 3377.1689\n",
      "(107673, 10) (10,) 144.1\n",
      "(107673, 10) 144.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 144.1\n",
      "(18846, 100, 10) 144.8\n",
      "(18846, 100, 10) 144.9\n",
      "(1884600, 10) 144.9\n",
      "(10,) 144.9\n",
      "(107673, 10) 145.0\n",
      "(107673, 10) 145.0\n",
      "Iteration [28/1000], update diff norm: 0.0043, perplexity: 3358.3386\n",
      "(107673, 10) (10,) 148.5\n",
      "(107673, 10) 148.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 148.6\n",
      "(18846, 100, 10) 149.2\n",
      "(18846, 100, 10) 149.2\n",
      "(1884600, 10) 149.3\n",
      "(10,) 149.3\n",
      "(107673, 10) 149.4\n",
      "(107673, 10) 149.4\n",
      "Iteration [29/1000], update diff norm: 0.0040, perplexity: 3341.6267\n",
      "(107673, 10) (10,) 154.0\n",
      "(107673, 10) 154.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 154.0\n",
      "(18846, 100, 10) 154.7\n",
      "(18846, 100, 10) 154.7\n",
      "(1884600, 10) 154.8\n",
      "(10,) 154.8\n",
      "(107673, 10) 154.8\n",
      "(107673, 10) 154.8\n",
      "Iteration [30/1000], update diff norm: 0.0037, perplexity: 3326.8567\n",
      "(107673, 10) (10,) 159.6\n",
      "(107673, 10) 159.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 159.6\n",
      "(18846, 100, 10) 160.3\n",
      "(18846, 100, 10) 160.3\n",
      "(1884600, 10) 160.4\n",
      "(10,) 160.4\n",
      "(107673, 10) 160.5\n",
      "(107673, 10) 160.5\n",
      "Iteration [31/1000], update diff norm: 0.0034, perplexity: 3313.7285\n",
      "(107673, 10) (10,) 164.1\n",
      "(107673, 10) 164.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 164.1\n",
      "(18846, 100, 10) 164.8\n",
      "(18846, 100, 10) 164.8\n",
      "(1884600, 10) 164.9\n",
      "(10,) 164.9\n",
      "(107673, 10) 165.0\n",
      "(107673, 10) 165.0\n",
      "Iteration [32/1000], update diff norm: 0.0032, perplexity: 3302.1099\n",
      "(107673, 10) (10,) 169.5\n",
      "(107673, 10) 169.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 169.5\n",
      "(18846, 100, 10) 170.2\n",
      "(18846, 100, 10) 170.2\n",
      "(1884600, 10) 170.3\n",
      "(10,) 170.3\n",
      "(107673, 10) 170.3\n",
      "(107673, 10) 170.3\n",
      "Iteration [33/1000], update diff norm: 0.0030, perplexity: 3291.8594\n",
      "(107673, 10) (10,) 174.7\n",
      "(107673, 10) 174.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 174.7\n",
      "(18846, 100, 10) 175.4\n",
      "(18846, 100, 10) 175.4\n",
      "(1884600, 10) 175.5\n",
      "(10,) 175.5\n",
      "(107673, 10) 175.5\n",
      "(107673, 10) 175.5\n",
      "Iteration [34/1000], update diff norm: 0.0029, perplexity: 3282.8650\n",
      "(107673, 10) (10,) 179.1\n",
      "(107673, 10) 179.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 179.1\n",
      "(18846, 100, 10) 179.8\n",
      "(18846, 100, 10) 179.8\n",
      "(1884600, 10) 179.9\n",
      "(10,) 179.9\n",
      "(107673, 10) 180.0\n",
      "(107673, 10) 180.0\n",
      "Iteration [35/1000], update diff norm: 0.0027, perplexity: 3274.8286\n",
      "(107673, 10) (10,) 185.0\n",
      "(107673, 10) 185.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 185.1\n",
      "(18846, 100, 10) 185.8\n",
      "(18846, 100, 10) 185.9\n",
      "(1884600, 10) 185.9\n",
      "(10,) 185.9\n",
      "(107673, 10) 186.0\n",
      "(107673, 10) 186.0\n",
      "Iteration [36/1000], update diff norm: 0.0025, perplexity: 3267.5039\n",
      "(107673, 10) (10,) 189.8\n",
      "(107673, 10) 189.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 189.9\n",
      "(18846, 100, 10) 190.5\n",
      "(18846, 100, 10) 190.6\n",
      "(1884600, 10) 190.6\n",
      "(10,) 190.6\n",
      "(107673, 10) 190.7\n",
      "(107673, 10) 190.7\n",
      "Iteration [37/1000], update diff norm: 0.0023, perplexity: 3260.8359\n",
      "(107673, 10) (10,) 195.8\n",
      "(107673, 10) 195.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 195.9\n",
      "(18846, 100, 10) 196.5\n",
      "(18846, 100, 10) 196.5\n",
      "(1884600, 10) 196.6\n",
      "(10,) 196.6\n",
      "(107673, 10) 196.6\n",
      "(107673, 10) 196.6\n",
      "Iteration [38/1000], update diff norm: 0.0021, perplexity: 3254.8643\n",
      "(107673, 10) (10,) 200.5\n",
      "(107673, 10) 200.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 200.6\n",
      "(18846, 100, 10) 201.2\n",
      "(18846, 100, 10) 201.2\n",
      "(1884600, 10) 201.3\n",
      "(10,) 201.3\n",
      "(107673, 10) 201.4\n",
      "(107673, 10) 201.4\n",
      "Iteration [39/1000], update diff norm: 0.0021, perplexity: 3249.3655\n",
      "(107673, 10) (10,) 204.9\n",
      "(107673, 10) 204.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 205.0\n",
      "(18846, 100, 10) 205.7\n",
      "(18846, 100, 10) 205.7\n",
      "(1884600, 10) 205.8\n",
      "(10,) 205.8\n",
      "(107673, 10) 205.9\n",
      "(107673, 10) 205.9\n",
      "Iteration [40/1000], update diff norm: 0.0020, perplexity: 3244.1853\n",
      "(107673, 10) (10,) 209.8\n",
      "(107673, 10) 209.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 209.8\n",
      "(18846, 100, 10) 210.5\n",
      "(18846, 100, 10) 210.5\n",
      "(1884600, 10) 210.6\n",
      "(10,) 210.6\n",
      "(107673, 10) 210.7\n",
      "(107673, 10) 210.7\n",
      "Iteration [41/1000], update diff norm: 0.0019, perplexity: 3239.4490\n",
      "(107673, 10) (10,) 215.7\n",
      "(107673, 10) 215.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 215.7\n",
      "(18846, 100, 10) 216.5\n",
      "(18846, 100, 10) 216.5\n",
      "(1884600, 10) 216.6\n",
      "(10,) 216.6\n",
      "(107673, 10) 216.6\n",
      "(107673, 10) 216.6\n",
      "Iteration [42/1000], update diff norm: 0.0018, perplexity: 3235.1421\n",
      "(107673, 10) (10,) 221.4\n",
      "(107673, 10) 221.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 221.5\n",
      "(18846, 100, 10) 222.2\n",
      "(18846, 100, 10) 222.2\n",
      "(1884600, 10) 222.3\n",
      "(10,) 222.3\n",
      "(107673, 10) 222.4\n",
      "(107673, 10) 222.4\n",
      "Iteration [43/1000], update diff norm: 0.0016, perplexity: 3231.1738\n",
      "(107673, 10) (10,) 226.4\n",
      "(107673, 10) 226.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 226.4\n",
      "(18846, 100, 10) 227.1\n",
      "(18846, 100, 10) 227.1\n",
      "(1884600, 10) 227.2\n",
      "(10,) 227.2\n",
      "(107673, 10) 227.3\n",
      "(107673, 10) 227.3\n",
      "Iteration [44/1000], update diff norm: 0.0015, perplexity: 3227.6719\n",
      "(107673, 10) (10,) 231.0\n",
      "(107673, 10) 231.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 231.0\n",
      "(18846, 100, 10) 231.7\n",
      "(18846, 100, 10) 231.7\n",
      "(1884600, 10) 231.8\n",
      "(10,) 231.8\n",
      "(107673, 10) 231.9\n",
      "(107673, 10) 231.9\n",
      "Iteration [45/1000], update diff norm: 0.0015, perplexity: 3224.4971\n",
      "(107673, 10) (10,) 237.2\n",
      "(107673, 10) 237.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 237.2\n",
      "(18846, 100, 10) 237.9\n",
      "(18846, 100, 10) 238.0\n",
      "(1884600, 10) 238.0\n",
      "(10,) 238.0\n",
      "(107673, 10) 238.1\n",
      "(107673, 10) 238.1\n",
      "Iteration [46/1000], update diff norm: 0.0013, perplexity: 3221.5247\n",
      "(107673, 10) (10,) 242.0\n",
      "(107673, 10) 242.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 242.0\n",
      "(18846, 100, 10) 242.6\n",
      "(18846, 100, 10) 242.6\n",
      "(1884600, 10) 242.7\n",
      "(10,) 242.7\n",
      "(107673, 10) 242.8\n",
      "(107673, 10) 242.8\n",
      "Iteration [47/1000], update diff norm: 0.0013, perplexity: 3218.7148\n",
      "(107673, 10) (10,) 248.1\n",
      "(107673, 10) 248.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 248.1\n",
      "(18846, 100, 10) 248.8\n",
      "(18846, 100, 10) 248.8\n",
      "(1884600, 10) 248.9\n",
      "(10,) 248.9\n",
      "(107673, 10) 248.9\n",
      "(107673, 10) 248.9\n",
      "Iteration [48/1000], update diff norm: 0.0012, perplexity: 3216.0391\n",
      "(107673, 10) (10,) 252.5\n",
      "(107673, 10) 252.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 252.5\n",
      "(18846, 100, 10) 253.2\n",
      "(18846, 100, 10) 253.2\n",
      "(1884600, 10) 253.2\n",
      "(10,) 253.2\n",
      "(107673, 10) 253.3\n",
      "(107673, 10) 253.3\n",
      "Iteration [49/1000], update diff norm: 0.0012, perplexity: 3213.5068\n",
      "(107673, 10) (10,) 258.9\n",
      "(107673, 10) 258.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 259.0\n",
      "(18846, 100, 10) 259.6\n",
      "(18846, 100, 10) 259.7\n",
      "(1884600, 10) 259.7\n",
      "(10,) 259.7\n",
      "(107673, 10) 259.8\n",
      "(107673, 10) 259.8\n",
      "Iteration [50/1000], update diff norm: 0.0012, perplexity: 3211.1294\n",
      "(107673, 10) (10,) 263.5\n",
      "(107673, 10) 263.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 263.6\n",
      "(18846, 100, 10) 264.3\n",
      "(18846, 100, 10) 264.3\n",
      "(1884600, 10) 264.4\n",
      "(10,) 264.4\n",
      "(107673, 10) 264.4\n",
      "(107673, 10) 264.4\n",
      "Iteration [51/1000], update diff norm: 0.0012, perplexity: 3208.9038\n",
      "(107673, 10) (10,) 269.7\n",
      "(107673, 10) 269.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 269.7\n",
      "(18846, 100, 10) 270.4\n",
      "(18846, 100, 10) 270.4\n",
      "(1884600, 10) 270.4\n",
      "(10,) 270.4\n",
      "(107673, 10) 270.5\n",
      "(107673, 10) 270.5\n",
      "Iteration [52/1000], update diff norm: 0.0011, perplexity: 3206.8328\n",
      "(107673, 10) (10,) 275.1\n",
      "(107673, 10) 275.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 275.1\n",
      "(18846, 100, 10) 275.8\n",
      "(18846, 100, 10) 275.8\n",
      "(1884600, 10) 275.9\n",
      "(10,) 275.9\n",
      "(107673, 10) 275.9\n",
      "(107673, 10) 275.9\n",
      "Iteration [53/1000], update diff norm: 0.0011, perplexity: 3204.8823\n",
      "(107673, 10) (10,) 280.6\n",
      "(107673, 10) 280.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 280.7\n",
      "(18846, 100, 10) 281.3\n",
      "(18846, 100, 10) 281.3\n",
      "(1884600, 10) 281.4\n",
      "(10,) 281.4\n",
      "(107673, 10) 281.5\n",
      "(107673, 10) 281.5\n",
      "Iteration [54/1000], update diff norm: 0.0010, perplexity: 3203.0581\n",
      "(107673, 10) (10,) 286.7\n",
      "(107673, 10) 286.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 286.7\n",
      "(18846, 100, 10) 287.4\n",
      "(18846, 100, 10) 287.4\n",
      "(1884600, 10) 287.5\n",
      "(10,) 287.5\n",
      "(107673, 10) 287.5\n",
      "(107673, 10) 287.5\n",
      "Iteration [55/1000], update diff norm: 0.0010, perplexity: 3201.3479\n",
      "CPU times: user 8min 36s, sys: 6min 16s, total: 14min 52s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = ContextTopicModel(\n",
    "    ctx_len=10,\n",
    "    max_len=maxlen,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    n_topics=10,\n",
    ")\n",
    "model.fit(dataset.data, test_data=bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107673, 10) (10,) 0.0\n",
      "(107673, 10) 0.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 0.0\n",
      "(18846, 100, 10) 0.7\n",
      "(18846, 100, 10) 0.7\n",
      "(1884600, 10) 0.8\n",
      "(10,) 0.8\n",
      "(107673, 10) 0.9\n",
      "(107673, 10) 0.9\n",
      "Iteration [0/1000], update diff norm: 0.4974, perplexity: 5963.5566\n",
      "(107673, 10) (10,) 6.0\n",
      "(107673, 10) 6.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 6.0\n",
      "(18846, 100, 10) 6.8\n",
      "(18846, 100, 10) 6.8\n",
      "(1884600, 10) 6.9\n",
      "(10,) 6.9\n",
      "(107673, 10) 6.9\n",
      "(107673, 10) 7.0\n",
      "Iteration [1/1000], update diff norm: 0.0717, perplexity: 5955.3384\n",
      "(107673, 10) (10,) 11.1\n",
      "(107673, 10) 11.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 11.1\n",
      "(18846, 100, 10) 11.8\n",
      "(18846, 100, 10) 11.8\n",
      "(1884600, 10) 11.9\n",
      "(10,) 11.9\n",
      "(107673, 10) 12.0\n",
      "(107673, 10) 12.0\n",
      "Iteration [2/1000], update diff norm: 0.0862, perplexity: 5840.1572\n",
      "(107673, 10) (10,) 16.5\n",
      "(107673, 10) 16.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 16.6\n",
      "(18846, 100, 10) 17.2\n",
      "(18846, 100, 10) 17.3\n",
      "(1884600, 10) 17.3\n",
      "(10,) 17.3\n",
      "(107673, 10) 17.4\n",
      "(107673, 10) 17.4\n",
      "Iteration [3/1000], update diff norm: 0.0936, perplexity: 5675.1011\n",
      "(107673, 10) (10,) 21.5\n",
      "(107673, 10) 21.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 21.5\n",
      "(18846, 100, 10) 22.2\n",
      "(18846, 100, 10) 22.2\n",
      "(1884600, 10) 22.3\n",
      "(10,) 22.3\n",
      "(107673, 10) 22.4\n",
      "(107673, 10) 22.4\n",
      "Iteration [4/1000], update diff norm: 0.0956, perplexity: 5482.9360\n",
      "(107673, 10) (10,) 27.8\n",
      "(107673, 10) 27.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 27.9\n",
      "(18846, 100, 10) 28.5\n",
      "(18846, 100, 10) 28.6\n",
      "(1884600, 10) 28.6\n",
      "(10,) 28.6\n",
      "(107673, 10) 28.7\n",
      "(107673, 10) 28.7\n",
      "Iteration [5/1000], update diff norm: 0.0954, perplexity: 5291.8701\n",
      "(107673, 10) (10,) 33.9\n",
      "(107673, 10) 33.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 33.9\n",
      "(18846, 100, 10) 34.6\n",
      "(18846, 100, 10) 34.6\n",
      "(1884600, 10) 34.7\n",
      "(10,) 34.7\n",
      "(107673, 10) 34.7\n",
      "(107673, 10) 34.7\n",
      "Iteration [6/1000], update diff norm: 0.0877, perplexity: 5110.2393\n",
      "(107673, 10) (10,) 39.1\n",
      "(107673, 10) 39.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 39.1\n",
      "(18846, 100, 10) 39.7\n",
      "(18846, 100, 10) 39.8\n",
      "(1884600, 10) 39.8\n",
      "(10,) 39.8\n",
      "(107673, 10) 39.9\n",
      "(107673, 10) 39.9\n",
      "Iteration [7/1000], update diff norm: 0.0766, perplexity: 4938.0342\n",
      "(107673, 10) (10,) 45.1\n",
      "(107673, 10) 45.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 45.1\n",
      "(18846, 100, 10) 45.8\n",
      "(18846, 100, 10) 45.8\n",
      "(1884600, 10) 45.9\n",
      "(10,) 45.9\n",
      "(107673, 10) 46.0\n",
      "(107673, 10) 46.0\n",
      "Iteration [8/1000], update diff norm: 0.0785, perplexity: 4773.9082\n",
      "(107673, 10) (10,) 50.0\n",
      "(107673, 10) 50.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 50.0\n",
      "(18846, 100, 10) 50.8\n",
      "(18846, 100, 10) 50.8\n",
      "(1884600, 10) 50.9\n",
      "(10,) 50.9\n",
      "(107673, 10) 50.9\n",
      "(107673, 10) 51.0\n",
      "Iteration [9/1000], update diff norm: 0.0979, perplexity: 4614.8496\n",
      "(107673, 10) (10,) 56.4\n",
      "(107673, 10) 56.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 56.5\n",
      "(18846, 100, 10) 57.1\n",
      "(18846, 100, 10) 57.2\n",
      "(1884600, 10) 57.2\n",
      "(10,) 57.2\n",
      "(107673, 10) 57.3\n",
      "(107673, 10) 57.3\n",
      "Iteration [10/1000], update diff norm: 0.1197, perplexity: 4459.5811\n",
      "(107673, 10) (10,) 62.0\n",
      "(107673, 10) 62.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 62.0\n",
      "(18846, 100, 10) 62.7\n",
      "(18846, 100, 10) 62.7\n",
      "(1884600, 10) 62.8\n",
      "(10,) 62.8\n",
      "(107673, 10) 62.8\n",
      "(107673, 10) 62.8\n",
      "Iteration [11/1000], update diff norm: 0.1209, perplexity: 4310.7939\n",
      "(107673, 10) (10,) 68.1\n",
      "(107673, 10) 68.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 68.1\n",
      "(18846, 100, 10) 68.7\n",
      "(18846, 100, 10) 68.8\n",
      "(1884600, 10) 68.8\n",
      "(10,) 68.8\n",
      "(107673, 10) 68.9\n",
      "(107673, 10) 68.9\n",
      "Iteration [12/1000], update diff norm: 0.0924, perplexity: 4170.6528\n",
      "(107673, 10) (10,) 74.5\n",
      "(107673, 10) 74.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 74.5\n",
      "(18846, 100, 10) 75.2\n",
      "(18846, 100, 10) 75.2\n",
      "(1884600, 10) 75.2\n",
      "(10,) 75.2\n",
      "(107673, 10) 75.3\n",
      "(107673, 10) 75.3\n",
      "Iteration [13/1000], update diff norm: 0.0525, perplexity: 4036.1179\n",
      "(107673, 10) (10,) 79.0\n",
      "(107673, 10) 79.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 79.1\n",
      "(18846, 100, 10) 79.7\n",
      "(18846, 100, 10) 79.8\n",
      "(1884600, 10) 79.8\n",
      "(10,) 79.8\n",
      "(107673, 10) 79.9\n",
      "(107673, 10) 79.9\n",
      "Iteration [14/1000], update diff norm: 0.0247, perplexity: 3904.8428\n",
      "(107673, 10) (10,) 84.8\n",
      "(107673, 10) 84.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 84.8\n",
      "(18846, 100, 10) 85.4\n",
      "(18846, 100, 10) 85.5\n",
      "(1884600, 10) 85.5\n",
      "(10,) 85.5\n",
      "(107673, 10) 85.6\n",
      "(107673, 10) 85.6\n",
      "Iteration [15/1000], update diff norm: 0.0131, perplexity: 3779.5420\n",
      "(107673, 10) (10,) 91.0\n",
      "(107673, 10) 91.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 91.1\n",
      "(18846, 100, 10) 91.7\n",
      "(18846, 100, 10) 91.7\n",
      "(1884600, 10) 91.8\n",
      "(10,) 91.8\n",
      "(107673, 10) 91.9\n",
      "(107673, 10) 91.9\n",
      "Iteration [16/1000], update diff norm: 0.0098, perplexity: 3664.8230\n",
      "(107673, 10) (10,) 96.0\n",
      "(107673, 10) 96.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 96.0\n",
      "(18846, 100, 10) 96.7\n",
      "(18846, 100, 10) 96.7\n",
      "(1884600, 10) 96.7\n",
      "(10,) 96.7\n",
      "(107673, 10) 96.8\n",
      "(107673, 10) 96.8\n",
      "Iteration [17/1000], update diff norm: 0.0087, perplexity: 3563.0474\n",
      "(107673, 10) (10,) 102.2\n",
      "(107673, 10) 102.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 102.2\n",
      "(18846, 100, 10) 102.9\n",
      "(18846, 100, 10) 102.9\n",
      "(1884600, 10) 103.0\n",
      "(10,) 103.0\n",
      "(107673, 10) 103.1\n",
      "(107673, 10) 103.1\n",
      "Iteration [18/1000], update diff norm: 0.0080, perplexity: 3474.2881\n",
      "(107673, 10) (10,) 108.1\n",
      "(107673, 10) 108.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 108.2\n",
      "(18846, 100, 10) 108.9\n",
      "(18846, 100, 10) 108.9\n",
      "(1884600, 10) 109.0\n",
      "(10,) 109.0\n",
      "(107673, 10) 109.0\n",
      "(107673, 10) 109.0\n",
      "Iteration [19/1000], update diff norm: 0.0075, perplexity: 3397.4492\n",
      "(107673, 10) (10,) 114.7\n",
      "(107673, 10) 114.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 114.8\n",
      "(18846, 100, 10) 115.4\n",
      "(18846, 100, 10) 115.4\n",
      "(1884600, 10) 115.5\n",
      "(10,) 115.5\n",
      "(107673, 10) 115.6\n",
      "(107673, 10) 115.6\n",
      "Iteration [20/1000], update diff norm: 0.0070, perplexity: 3331.1584\n",
      "(107673, 10) (10,) 119.5\n",
      "(107673, 10) 119.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 119.6\n",
      "(18846, 100, 10) 120.2\n",
      "(18846, 100, 10) 120.3\n",
      "(1884600, 10) 120.3\n",
      "(10,) 120.3\n",
      "(107673, 10) 120.4\n",
      "(107673, 10) 120.4\n",
      "Iteration [21/1000], update diff norm: 0.0066, perplexity: 3273.9888\n",
      "(107673, 10) (10,) 124.2\n",
      "(107673, 10) 124.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 124.2\n",
      "(18846, 100, 10) 125.0\n",
      "(18846, 100, 10) 125.0\n",
      "(1884600, 10) 125.1\n",
      "(10,) 125.1\n",
      "(107673, 10) 125.2\n",
      "(107673, 10) 125.2\n",
      "Iteration [22/1000], update diff norm: 0.0063, perplexity: 3224.5676\n",
      "(107673, 10) (10,) 130.5\n",
      "(107673, 10) 130.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 130.5\n",
      "(18846, 100, 10) 131.2\n",
      "(18846, 100, 10) 131.3\n",
      "(1884600, 10) 131.3\n",
      "(10,) 131.3\n",
      "(107673, 10) 131.4\n",
      "(107673, 10) 131.4\n",
      "Iteration [23/1000], update diff norm: 0.0060, perplexity: 3181.6494\n",
      "(107673, 10) (10,) 135.0\n",
      "(107673, 10) 135.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 135.1\n",
      "(18846, 100, 10) 135.7\n",
      "(18846, 100, 10) 135.8\n",
      "(1884600, 10) 135.8\n",
      "(10,) 135.8\n",
      "(107673, 10) 135.9\n",
      "(107673, 10) 135.9\n",
      "Iteration [24/1000], update diff norm: 0.0058, perplexity: 3144.2402\n",
      "(107673, 10) (10,) 141.4\n",
      "(107673, 10) 141.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 141.4\n",
      "(18846, 100, 10) 142.1\n",
      "(18846, 100, 10) 142.1\n",
      "(1884600, 10) 142.2\n",
      "(10,) 142.2\n",
      "(107673, 10) 142.3\n",
      "(107673, 10) 142.3\n",
      "Iteration [25/1000], update diff norm: 0.0055, perplexity: 3111.5293\n",
      "(107673, 10) (10,) 146.1\n",
      "(107673, 10) 146.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 146.1\n",
      "(18846, 100, 10) 146.8\n",
      "(18846, 100, 10) 146.8\n",
      "(1884600, 10) 146.9\n",
      "(10,) 146.9\n",
      "(107673, 10) 147.0\n",
      "(107673, 10) 147.0\n",
      "Iteration [26/1000], update diff norm: 0.0053, perplexity: 3082.8105\n",
      "(107673, 10) (10,) 151.8\n",
      "(107673, 10) 151.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 151.8\n",
      "(18846, 100, 10) 152.5\n",
      "(18846, 100, 10) 152.5\n",
      "(1884600, 10) 152.6\n",
      "(10,) 152.6\n",
      "(107673, 10) 152.7\n",
      "(107673, 10) 152.7\n",
      "Iteration [27/1000], update diff norm: 0.0051, perplexity: 3057.5542\n",
      "(107673, 10) (10,) 158.1\n",
      "(107673, 10) 158.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 158.1\n",
      "(18846, 100, 10) 158.9\n",
      "(18846, 100, 10) 158.9\n",
      "(1884600, 10) 158.9\n",
      "(10,) 159.0\n",
      "(107673, 10) 159.0\n",
      "(107673, 10) 159.0\n",
      "Iteration [28/1000], update diff norm: 0.0049, perplexity: 3035.2217\n",
      "(107673, 10) (10,) 162.7\n",
      "(107673, 10) 162.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 162.7\n",
      "(18846, 100, 10) 163.4\n",
      "(18846, 100, 10) 163.4\n",
      "(1884600, 10) 163.5\n",
      "(10,) 163.5\n",
      "(107673, 10) 163.6\n",
      "(107673, 10) 163.6\n",
      "Iteration [29/1000], update diff norm: 0.0047, perplexity: 3015.3948\n",
      "(107673, 10) (10,) 169.0\n",
      "(107673, 10) 169.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 169.1\n",
      "(18846, 100, 10) 169.8\n",
      "(18846, 100, 10) 169.8\n",
      "(1884600, 10) 169.9\n",
      "(10,) 169.9\n",
      "(107673, 10) 170.0\n",
      "(107673, 10) 170.0\n",
      "Iteration [30/1000], update diff norm: 0.0045, perplexity: 2997.7153\n",
      "(107673, 10) (10,) 173.8\n",
      "(107673, 10) 173.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 173.8\n",
      "(18846, 100, 10) 174.4\n",
      "(18846, 100, 10) 174.4\n",
      "(1884600, 10) 174.5\n",
      "(10,) 174.5\n",
      "(107673, 10) 174.6\n",
      "(107673, 10) 174.6\n",
      "Iteration [31/1000], update diff norm: 0.0043, perplexity: 2981.9673\n",
      "(107673, 10) (10,) 180.1\n",
      "(107673, 10) 180.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 180.1\n",
      "(18846, 100, 10) 180.8\n",
      "(18846, 100, 10) 180.9\n",
      "(1884600, 10) 180.9\n",
      "(10,) 180.9\n",
      "(107673, 10) 181.0\n",
      "(107673, 10) 181.0\n",
      "Iteration [32/1000], update diff norm: 0.0042, perplexity: 2967.9321\n",
      "(107673, 10) (10,) 185.0\n",
      "(107673, 10) 185.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 185.0\n",
      "(18846, 100, 10) 185.7\n",
      "(18846, 100, 10) 185.7\n",
      "(1884600, 10) 185.8\n",
      "(10,) 185.8\n",
      "(107673, 10) 185.9\n",
      "(107673, 10) 185.9\n",
      "Iteration [33/1000], update diff norm: 0.0041, perplexity: 2955.3857\n",
      "(107673, 10) (10,) 191.5\n",
      "(107673, 10) 191.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 191.6\n",
      "(18846, 100, 10) 192.2\n",
      "(18846, 100, 10) 192.2\n",
      "(1884600, 10) 192.3\n",
      "(10,) 192.3\n",
      "(107673, 10) 192.4\n",
      "(107673, 10) 192.4\n",
      "Iteration [34/1000], update diff norm: 0.0040, perplexity: 2944.1152\n",
      "(107673, 10) (10,) 196.3\n",
      "(107673, 10) 196.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 196.3\n",
      "(18846, 100, 10) 197.1\n",
      "(18846, 100, 10) 197.1\n",
      "(1884600, 10) 197.1\n",
      "(10,) 197.1\n",
      "(107673, 10) 197.2\n",
      "(107673, 10) 197.2\n",
      "Iteration [35/1000], update diff norm: 0.0039, perplexity: 2934.0024\n",
      "(107673, 10) (10,) 202.2\n",
      "(107673, 10) 202.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 202.2\n",
      "(18846, 100, 10) 202.9\n",
      "(18846, 100, 10) 202.9\n",
      "(1884600, 10) 203.0\n",
      "(10,) 203.0\n",
      "(107673, 10) 203.1\n",
      "(107673, 10) 203.1\n",
      "Iteration [36/1000], update diff norm: 0.0038, perplexity: 2924.9492\n",
      "(107673, 10) (10,) 207.8\n",
      "(107673, 10) 207.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 207.9\n",
      "(18846, 100, 10) 208.6\n",
      "(18846, 100, 10) 208.6\n",
      "(1884600, 10) 208.7\n",
      "(10,) 208.7\n",
      "(107673, 10) 208.8\n",
      "(107673, 10) 208.8\n",
      "Iteration [37/1000], update diff norm: 0.0037, perplexity: 2916.8193\n",
      "(107673, 10) (10,) 214.1\n",
      "(107673, 10) 214.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 214.2\n",
      "(18846, 100, 10) 214.8\n",
      "(18846, 100, 10) 214.9\n",
      "(1884600, 10) 215.0\n",
      "(10,) 215.0\n",
      "(107673, 10) 215.1\n",
      "(107673, 10) 215.1\n",
      "Iteration [38/1000], update diff norm: 0.0036, perplexity: 2909.4365\n",
      "(107673, 10) (10,) 218.6\n",
      "(107673, 10) 218.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 218.7\n",
      "(18846, 100, 10) 219.3\n",
      "(18846, 100, 10) 219.4\n",
      "(1884600, 10) 219.4\n",
      "(10,) 219.4\n",
      "(107673, 10) 219.5\n",
      "(107673, 10) 219.5\n",
      "Iteration [39/1000], update diff norm: 0.0035, perplexity: 2902.6494\n",
      "(107673, 10) (10,) 224.9\n",
      "(107673, 10) 225.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 225.0\n",
      "(18846, 100, 10) 225.7\n",
      "(18846, 100, 10) 225.7\n",
      "(1884600, 10) 225.7\n",
      "(10,) 225.7\n",
      "(107673, 10) 225.8\n",
      "(107673, 10) 225.8\n",
      "Iteration [40/1000], update diff norm: 0.0034, perplexity: 2896.4041\n",
      "(107673, 10) (10,) 229.4\n",
      "(107673, 10) 229.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 229.5\n",
      "(18846, 100, 10) 230.1\n",
      "(18846, 100, 10) 230.1\n",
      "(1884600, 10) 230.2\n",
      "(10,) 230.2\n",
      "(107673, 10) 230.3\n",
      "(107673, 10) 230.3\n",
      "Iteration [41/1000], update diff norm: 0.0034, perplexity: 2890.5884\n",
      "(107673, 10) (10,) 234.9\n",
      "(107673, 10) 235.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 235.0\n",
      "(18846, 100, 10) 235.7\n",
      "(18846, 100, 10) 235.7\n",
      "(1884600, 10) 235.8\n",
      "(10,) 235.8\n",
      "(107673, 10) 235.9\n",
      "(107673, 10) 235.9\n",
      "Iteration [42/1000], update diff norm: 0.0033, perplexity: 2885.1946\n",
      "(107673, 10) (10,) 240.3\n",
      "(107673, 10) 240.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 240.4\n",
      "(18846, 100, 10) 241.1\n",
      "(18846, 100, 10) 241.1\n",
      "(1884600, 10) 241.2\n",
      "(10,) 241.2\n",
      "(107673, 10) 241.3\n",
      "(107673, 10) 241.3\n",
      "Iteration [43/1000], update diff norm: 0.0032, perplexity: 2880.1362\n",
      "(107673, 10) (10,) 245.0\n",
      "(107673, 10) 245.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 245.0\n",
      "(18846, 100, 10) 245.7\n",
      "(18846, 100, 10) 245.7\n",
      "(1884600, 10) 245.8\n",
      "(10,) 245.8\n",
      "(107673, 10) 245.9\n",
      "(107673, 10) 245.9\n",
      "Iteration [44/1000], update diff norm: 0.0031, perplexity: 2875.4062\n",
      "(107673, 10) (10,) 250.1\n",
      "(107673, 10) 250.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 250.1\n",
      "(18846, 100, 10) 250.8\n",
      "(18846, 100, 10) 250.9\n",
      "(1884600, 10) 250.9\n",
      "(10,) 250.9\n",
      "(107673, 10) 251.0\n",
      "(107673, 10) 251.0\n",
      "Iteration [45/1000], update diff norm: 0.0030, perplexity: 2870.9836\n",
      "(107673, 10) (10,) 255.8\n",
      "(107673, 10) 255.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 255.8\n",
      "(18846, 100, 10) 256.5\n",
      "(18846, 100, 10) 256.6\n",
      "(1884600, 10) 256.6\n",
      "(10,) 256.6\n",
      "(107673, 10) 256.7\n",
      "(107673, 10) 256.7\n",
      "Iteration [46/1000], update diff norm: 0.0029, perplexity: 2866.8276\n",
      "(107673, 10) (10,) 261.2\n",
      "(107673, 10) 261.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 261.3\n",
      "(18846, 100, 10) 262.0\n",
      "(18846, 100, 10) 262.0\n",
      "(1884600, 10) 262.1\n",
      "(10,) 262.1\n",
      "(107673, 10) 262.2\n",
      "(107673, 10) 262.2\n",
      "Iteration [47/1000], update diff norm: 0.0028, perplexity: 2862.9287\n",
      "(107673, 10) (10,) 267.0\n",
      "(107673, 10) 267.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 267.1\n",
      "(18846, 100, 10) 267.8\n",
      "(18846, 100, 10) 267.8\n",
      "(1884600, 10) 267.9\n",
      "(10,) 267.9\n",
      "(107673, 10) 268.0\n",
      "(107673, 10) 268.0\n",
      "Iteration [48/1000], update diff norm: 0.0028, perplexity: 2859.2466\n",
      "(107673, 10) (10,) 272.3\n",
      "(107673, 10) 272.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 272.3\n",
      "(18846, 100, 10) 273.1\n",
      "(18846, 100, 10) 273.1\n",
      "(1884600, 10) 273.2\n",
      "(10,) 273.2\n",
      "(107673, 10) 273.3\n",
      "(107673, 10) 273.3\n",
      "Iteration [49/1000], update diff norm: 0.0027, perplexity: 2855.8047\n",
      "(107673, 10) (10,) 277.9\n",
      "(107673, 10) 277.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 277.9\n",
      "(18846, 100, 10) 278.6\n",
      "(18846, 100, 10) 278.6\n",
      "(1884600, 10) 278.6\n",
      "(10,) 278.6\n",
      "(107673, 10) 278.7\n",
      "(107673, 10) 278.7\n",
      "Iteration [50/1000], update diff norm: 0.0026, perplexity: 2852.5276\n",
      "(107673, 10) (10,) 283.2\n",
      "(107673, 10) 283.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 283.2\n",
      "(18846, 100, 10) 284.0\n",
      "(18846, 100, 10) 284.0\n",
      "(1884600, 10) 284.1\n",
      "(10,) 284.1\n",
      "(107673, 10) 284.2\n",
      "(107673, 10) 284.2\n",
      "Iteration [51/1000], update diff norm: 0.0024, perplexity: 2849.4348\n",
      "(107673, 10) (10,) 288.7\n",
      "(107673, 10) 288.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 288.7\n",
      "(18846, 100, 10) 289.4\n",
      "(18846, 100, 10) 289.4\n",
      "(1884600, 10) 289.5\n",
      "(10,) 289.5\n",
      "(107673, 10) 289.6\n",
      "(107673, 10) 289.6\n",
      "Iteration [52/1000], update diff norm: 0.0023, perplexity: 2846.4976\n",
      "(107673, 10) (10,) 295.0\n",
      "(107673, 10) 295.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 295.0\n",
      "(18846, 100, 10) 295.7\n",
      "(18846, 100, 10) 295.7\n",
      "(1884600, 10) 295.7\n",
      "(10,) 295.7\n",
      "(107673, 10) 295.8\n",
      "(107673, 10) 295.8\n",
      "Iteration [53/1000], update diff norm: 0.0022, perplexity: 2843.6904\n",
      "(107673, 10) (10,) 300.1\n",
      "(107673, 10) 300.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 300.2\n",
      "(18846, 100, 10) 300.9\n",
      "(18846, 100, 10) 300.9\n",
      "(1884600, 10) 301.0\n",
      "(10,) 301.0\n",
      "(107673, 10) 301.1\n",
      "(107673, 10) 301.1\n",
      "Iteration [54/1000], update diff norm: 0.0021, perplexity: 2841.0151\n",
      "(107673, 10) (10,) 305.4\n",
      "(107673, 10) 305.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 305.5\n",
      "(18846, 100, 10) 306.1\n",
      "(18846, 100, 10) 306.2\n",
      "(1884600, 10) 306.2\n",
      "(10,) 306.2\n",
      "(107673, 10) 306.3\n",
      "(107673, 10) 306.3\n",
      "Iteration [55/1000], update diff norm: 0.0020, perplexity: 2838.5046\n",
      "(107673, 10) (10,) 310.1\n",
      "(107673, 10) 310.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 310.1\n",
      "(18846, 100, 10) 310.8\n",
      "(18846, 100, 10) 310.8\n",
      "(1884600, 10) 310.9\n",
      "(10,) 310.9\n",
      "(107673, 10) 311.0\n",
      "(107673, 10) 311.0\n",
      "Iteration [56/1000], update diff norm: 0.0020, perplexity: 2836.0950\n",
      "(107673, 10) (10,) 315.6\n",
      "(107673, 10) 315.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 315.6\n",
      "(18846, 100, 10) 316.3\n",
      "(18846, 100, 10) 316.4\n",
      "(1884600, 10) 316.4\n",
      "(10,) 316.4\n",
      "(107673, 10) 316.5\n",
      "(107673, 10) 316.5\n",
      "Iteration [57/1000], update diff norm: 0.0019, perplexity: 2833.8374\n",
      "(107673, 10) (10,) 322.0\n",
      "(107673, 10) 322.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 322.0\n",
      "(18846, 100, 10) 322.7\n",
      "(18846, 100, 10) 322.7\n",
      "(1884600, 10) 322.8\n",
      "(10,) 322.8\n",
      "(107673, 10) 322.9\n",
      "(107673, 10) 322.9\n",
      "Iteration [58/1000], update diff norm: 0.0018, perplexity: 2831.6411\n",
      "(107673, 10) (10,) 327.8\n",
      "(107673, 10) 327.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 327.9\n",
      "(18846, 100, 10) 328.5\n",
      "(18846, 100, 10) 328.6\n",
      "(1884600, 10) 328.6\n",
      "(10,) 328.6\n",
      "(107673, 10) 328.7\n",
      "(107673, 10) 328.7\n",
      "Iteration [59/1000], update diff norm: 0.0017, perplexity: 2829.4966\n",
      "(107673, 10) (10,) 332.5\n",
      "(107673, 10) 332.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 332.6\n",
      "(18846, 100, 10) 333.3\n",
      "(18846, 100, 10) 333.4\n",
      "(1884600, 10) 333.4\n",
      "(10,) 333.4\n",
      "(107673, 10) 333.5\n",
      "(107673, 10) 333.5\n",
      "Iteration [60/1000], update diff norm: 0.0016, perplexity: 2827.3872\n",
      "(107673, 10) (10,) 338.2\n",
      "(107673, 10) 338.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 338.3\n",
      "(18846, 100, 10) 339.0\n",
      "(18846, 100, 10) 339.0\n",
      "(1884600, 10) 339.0\n",
      "(10,) 339.1\n",
      "(107673, 10) 339.1\n",
      "(107673, 10) 339.1\n",
      "Iteration [61/1000], update diff norm: 0.0016, perplexity: 2825.2820\n",
      "(107673, 10) (10,) 342.6\n",
      "(107673, 10) 342.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 342.7\n",
      "(18846, 100, 10) 343.5\n",
      "(18846, 100, 10) 343.5\n",
      "(1884600, 10) 343.5\n",
      "(10,) 343.5\n",
      "(107673, 10) 343.6\n",
      "(107673, 10) 343.6\n",
      "Iteration [62/1000], update diff norm: 0.0015, perplexity: 2823.2000\n",
      "(107673, 10) (10,) 348.0\n",
      "(107673, 10) 348.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 348.0\n",
      "(18846, 100, 10) 348.7\n",
      "(18846, 100, 10) 348.8\n",
      "(1884600, 10) 348.8\n",
      "(10,) 348.8\n",
      "(107673, 10) 348.9\n",
      "(107673, 10) 348.9\n",
      "Iteration [63/1000], update diff norm: 0.0014, perplexity: 2821.1855\n",
      "(107673, 10) (10,) 353.5\n",
      "(107673, 10) 353.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 353.6\n",
      "(18846, 100, 10) 354.2\n",
      "(18846, 100, 10) 354.3\n",
      "(1884600, 10) 354.3\n",
      "(10,) 354.3\n",
      "(107673, 10) 354.4\n",
      "(107673, 10) 354.4\n",
      "Iteration [64/1000], update diff norm: 0.0014, perplexity: 2819.2007\n",
      "(107673, 10) (10,) 358.6\n",
      "(107673, 10) 358.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 358.6\n",
      "(18846, 100, 10) 359.3\n",
      "(18846, 100, 10) 359.3\n",
      "(1884600, 10) 359.4\n",
      "(10,) 359.4\n",
      "(107673, 10) 359.5\n",
      "(107673, 10) 359.5\n",
      "Iteration [65/1000], update diff norm: 0.0013, perplexity: 2817.2896\n",
      "(107673, 10) (10,) 363.9\n",
      "(107673, 10) 363.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 363.9\n",
      "(18846, 100, 10) 364.6\n",
      "(18846, 100, 10) 364.6\n",
      "(1884600, 10) 364.6\n",
      "(10,) 364.6\n",
      "(107673, 10) 364.7\n",
      "(107673, 10) 364.7\n",
      "Iteration [66/1000], update diff norm: 0.0013, perplexity: 2815.4739\n",
      "(107673, 10) (10,) 369.8\n",
      "(107673, 10) 369.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 369.8\n",
      "(18846, 100, 10) 370.5\n",
      "(18846, 100, 10) 370.6\n",
      "(1884600, 10) 370.6\n",
      "(10,) 370.6\n",
      "(107673, 10) 370.7\n",
      "(107673, 10) 370.7\n",
      "Iteration [67/1000], update diff norm: 0.0012, perplexity: 2813.7546\n",
      "(107673, 10) (10,) 375.0\n",
      "(107673, 10) 375.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 375.1\n",
      "(18846, 100, 10) 375.8\n",
      "(18846, 100, 10) 375.8\n",
      "(1884600, 10) 375.9\n",
      "(10,) 375.9\n",
      "(107673, 10) 376.0\n",
      "(107673, 10) 376.0\n",
      "Iteration [68/1000], update diff norm: 0.0012, perplexity: 2812.1611\n",
      "(107673, 10) (10,) 380.0\n",
      "(107673, 10) 380.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 380.1\n",
      "(18846, 100, 10) 380.8\n",
      "(18846, 100, 10) 380.8\n",
      "(1884600, 10) 380.9\n",
      "(10,) 380.9\n",
      "(107673, 10) 380.9\n",
      "(107673, 10) 380.9\n",
      "Iteration [69/1000], update diff norm: 0.0011, perplexity: 2810.6274\n",
      "(107673, 10) (10,) 385.8\n",
      "(107673, 10) 385.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 385.8\n",
      "(18846, 100, 10) 386.5\n",
      "(18846, 100, 10) 386.6\n",
      "(1884600, 10) 386.6\n",
      "(10,) 386.6\n",
      "(107673, 10) 386.7\n",
      "(107673, 10) 386.7\n",
      "Iteration [70/1000], update diff norm: 0.0011, perplexity: 2809.1458\n",
      "(107673, 10) (10,) 391.1\n",
      "(107673, 10) 391.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 391.1\n",
      "(18846, 100, 10) 391.8\n",
      "(18846, 100, 10) 391.8\n",
      "(1884600, 10) 391.9\n",
      "(10,) 391.9\n",
      "(107673, 10) 392.0\n",
      "(107673, 10) 392.0\n",
      "Iteration [71/1000], update diff norm: 0.0010, perplexity: 2807.7449\n",
      "(107673, 10) (10,) 396.2\n",
      "(107673, 10) 396.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 396.2\n",
      "(18846, 100, 10) 396.9\n",
      "(18846, 100, 10) 396.9\n",
      "(1884600, 10) 397.0\n",
      "(10,) 397.0\n",
      "(107673, 10) 397.1\n",
      "(107673, 10) 397.1\n",
      "Iteration [72/1000], update diff norm: 0.0010, perplexity: 2806.4092\n",
      "(107673, 10) (10,) 402.2\n",
      "(107673, 10) 402.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 402.3\n",
      "(18846, 100, 10) 402.9\n",
      "(18846, 100, 10) 402.9\n",
      "(1884600, 10) 403.0\n",
      "(10,) 403.0\n",
      "(107673, 10) 403.1\n",
      "(107673, 10) 403.1\n",
      "Iteration [73/1000], update diff norm: 0.0010, perplexity: 2805.1394\n",
      "CPU times: user 11min 26s, sys: 8min 45s, total: 20min 11s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = ContextTopicModelDebug(\n",
    "    ctx_len=100,\n",
    "    max_len=maxlen,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    n_topics=10,\n",
    ")\n",
    "model.fit(dataset.data, test_data=bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107673, 10) (10,) 0.0\n",
      "(107673, 10) 0.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 0.0\n",
      "(18846, 100, 10) 0.7\n",
      "(18846, 100, 10) 0.7\n",
      "(1884600, 10) 0.7\n",
      "(10,) 0.7\n",
      "(107673, 10) 0.8\n",
      "(107673, 10) 0.9\n",
      "Iteration [0/1000], update diff norm: 0.4942, perplexity: 5963.4033\n",
      "(107673, 10) (10,) 7.6\n",
      "(107673, 10) 7.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 7.7\n",
      "(18846, 100, 10) 8.3\n",
      "(18846, 100, 10) 8.3\n",
      "(1884600, 10) 8.4\n",
      "(10,) 8.4\n",
      "(107673, 10) 8.5\n",
      "(107673, 10) 8.5\n",
      "Iteration [1/1000], update diff norm: 0.0599, perplexity: 5956.3892\n",
      "(107673, 10) (10,) 14.9\n",
      "(107673, 10) 14.9\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 15.0\n",
      "(18846, 100, 10) 15.6\n",
      "(18846, 100, 10) 15.6\n",
      "(1884600, 10) 15.7\n",
      "(10,) 15.7\n",
      "(107673, 10) 15.8\n",
      "(107673, 10) 15.8\n",
      "Iteration [2/1000], update diff norm: 0.0722, perplexity: 5844.9102\n",
      "(107673, 10) (10,) 19.5\n",
      "(107673, 10) 19.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 19.5\n",
      "(18846, 100, 10) 20.3\n",
      "(18846, 100, 10) 20.3\n",
      "(1884600, 10) 20.4\n",
      "(10,) 20.4\n",
      "(107673, 10) 20.5\n",
      "(107673, 10) 20.5\n",
      "Iteration [3/1000], update diff norm: 0.0797, perplexity: 5682.1143\n",
      "(107673, 10) (10,) 26.0\n",
      "(107673, 10) 26.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 26.1\n",
      "(18846, 100, 10) 26.7\n",
      "(18846, 100, 10) 26.8\n",
      "(1884600, 10) 26.8\n",
      "(10,) 26.8\n",
      "(107673, 10) 26.9\n",
      "(107673, 10) 26.9\n",
      "Iteration [4/1000], update diff norm: 0.0811, perplexity: 5491.5547\n",
      "(107673, 10) (10,) 34.1\n",
      "(107673, 10) 34.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 34.2\n",
      "(18846, 100, 10) 34.9\n",
      "(18846, 100, 10) 34.9\n",
      "(1884600, 10) 35.0\n",
      "(10,) 35.0\n",
      "(107673, 10) 35.1\n",
      "(107673, 10) 35.1\n",
      "Iteration [5/1000], update diff norm: 0.0792, perplexity: 5306.1660\n",
      "(107673, 10) (10,) 41.1\n",
      "(107673, 10) 41.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 41.1\n",
      "(18846, 100, 10) 41.8\n",
      "(18846, 100, 10) 41.8\n",
      "(1884600, 10) 41.8\n",
      "(10,) 41.8\n",
      "(107673, 10) 41.9\n",
      "(107673, 10) 41.9\n",
      "Iteration [6/1000], update diff norm: 0.0743, perplexity: 5134.4834\n",
      "(107673, 10) (10,) 48.2\n",
      "(107673, 10) 48.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 48.3\n",
      "(18846, 100, 10) 48.9\n",
      "(18846, 100, 10) 48.9\n",
      "(1884600, 10) 49.0\n",
      "(10,) 49.0\n",
      "(107673, 10) 49.1\n",
      "(107673, 10) 49.1\n",
      "Iteration [7/1000], update diff norm: 0.0654, perplexity: 4974.5669\n",
      "(107673, 10) (10,) 55.0\n",
      "(107673, 10) 55.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 55.1\n",
      "(18846, 100, 10) 55.8\n",
      "(18846, 100, 10) 55.8\n",
      "(1884600, 10) 55.9\n",
      "(10,) 55.9\n",
      "(107673, 10) 56.0\n",
      "(107673, 10) 56.0\n",
      "Iteration [8/1000], update diff norm: 0.0599, perplexity: 4824.3164\n",
      "(107673, 10) (10,) 62.3\n",
      "(107673, 10) 62.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 62.4\n",
      "(18846, 100, 10) 63.1\n",
      "(18846, 100, 10) 63.1\n",
      "(1884600, 10) 63.1\n",
      "(10,) 63.1\n",
      "(107673, 10) 63.2\n",
      "(107673, 10) 63.2\n",
      "Iteration [9/1000], update diff norm: 0.0656, perplexity: 4678.5151\n",
      "(107673, 10) (10,) 69.8\n",
      "(107673, 10) 69.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 69.9\n",
      "(18846, 100, 10) 70.6\n",
      "(18846, 100, 10) 70.6\n",
      "(1884600, 10) 70.7\n",
      "(10,) 70.7\n",
      "(107673, 10) 70.8\n",
      "(107673, 10) 70.8\n",
      "Iteration [10/1000], update diff norm: 0.0805, perplexity: 4530.9585\n",
      "(107673, 10) (10,) 77.5\n",
      "(107673, 10) 77.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 77.5\n",
      "(18846, 100, 10) 78.2\n",
      "(18846, 100, 10) 78.3\n",
      "(1884600, 10) 78.3\n",
      "(10,) 78.3\n",
      "(107673, 10) 78.4\n",
      "(107673, 10) 78.4\n",
      "Iteration [11/1000], update diff norm: 0.0956, perplexity: 4378.9175\n",
      "(107673, 10) (10,) 84.4\n",
      "(107673, 10) 84.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 84.5\n",
      "(18846, 100, 10) 85.1\n",
      "(18846, 100, 10) 85.1\n",
      "(1884600, 10) 85.2\n",
      "(10,) 85.2\n",
      "(107673, 10) 85.3\n",
      "(107673, 10) 85.3\n",
      "Iteration [12/1000], update diff norm: 0.1001, perplexity: 4225.1025\n",
      "(107673, 10) (10,) 91.5\n",
      "(107673, 10) 91.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 91.6\n",
      "(18846, 100, 10) 92.2\n",
      "(18846, 100, 10) 92.3\n",
      "(1884600, 10) 92.3\n",
      "(10,) 92.3\n",
      "(107673, 10) 92.4\n",
      "(107673, 10) 92.4\n",
      "Iteration [13/1000], update diff norm: 0.0870, perplexity: 4075.5630\n",
      "(107673, 10) (10,) 98.2\n",
      "(107673, 10) 98.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 98.2\n",
      "(18846, 100, 10) 98.9\n",
      "(18846, 100, 10) 98.9\n",
      "(1884600, 10) 99.0\n",
      "(10,) 99.0\n",
      "(107673, 10) 99.1\n",
      "(107673, 10) 99.1\n",
      "Iteration [14/1000], update diff norm: 0.0615, perplexity: 3935.0298\n",
      "(107673, 10) (10,) 102.7\n",
      "(107673, 10) 102.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 102.7\n",
      "(18846, 100, 10) 103.4\n",
      "(18846, 100, 10) 103.5\n",
      "(1884600, 10) 103.5\n",
      "(10,) 103.5\n",
      "(107673, 10) 103.6\n",
      "(107673, 10) 103.6\n",
      "Iteration [15/1000], update diff norm: 0.0364, perplexity: 3805.4634\n",
      "(107673, 10) (10,) 109.7\n",
      "(107673, 10) 109.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 109.8\n",
      "(18846, 100, 10) 110.4\n",
      "(18846, 100, 10) 110.5\n",
      "(1884600, 10) 110.5\n",
      "(10,) 110.5\n",
      "(107673, 10) 110.6\n",
      "(107673, 10) 110.6\n",
      "Iteration [16/1000], update diff norm: 0.0199, perplexity: 3688.3042\n",
      "(107673, 10) (10,) 114.4\n",
      "(107673, 10) 114.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 114.4\n",
      "(18846, 100, 10) 115.2\n",
      "(18846, 100, 10) 115.2\n",
      "(1884600, 10) 115.2\n",
      "(10,) 115.2\n",
      "(107673, 10) 115.3\n",
      "(107673, 10) 115.3\n",
      "Iteration [17/1000], update diff norm: 0.0119, perplexity: 3584.1909\n",
      "(107673, 10) (10,) 121.4\n",
      "(107673, 10) 121.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 121.4\n",
      "(18846, 100, 10) 122.2\n",
      "(18846, 100, 10) 122.2\n",
      "(1884600, 10) 122.2\n",
      "(10,) 122.3\n",
      "(107673, 10) 122.3\n",
      "(107673, 10) 122.3\n",
      "Iteration [18/1000], update diff norm: 0.0090, perplexity: 3492.9890\n",
      "(107673, 10) (10,) 125.9\n",
      "(107673, 10) 126.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 126.0\n",
      "(18846, 100, 10) 126.7\n",
      "(18846, 100, 10) 126.7\n",
      "(1884600, 10) 126.8\n",
      "(10,) 126.8\n",
      "(107673, 10) 126.9\n",
      "(107673, 10) 126.9\n",
      "Iteration [19/1000], update diff norm: 0.0079, perplexity: 3413.6589\n",
      "(107673, 10) (10,) 133.1\n",
      "(107673, 10) 133.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 133.2\n",
      "(18846, 100, 10) 133.9\n",
      "(18846, 100, 10) 133.9\n",
      "(1884600, 10) 134.0\n",
      "(10,) 134.0\n",
      "(107673, 10) 134.1\n",
      "(107673, 10) 134.1\n",
      "Iteration [20/1000], update diff norm: 0.0072, perplexity: 3344.8884\n",
      "(107673, 10) (10,) 139.7\n",
      "(107673, 10) 139.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 139.7\n",
      "(18846, 100, 10) 140.3\n",
      "(18846, 100, 10) 140.3\n",
      "(1884600, 10) 140.4\n",
      "(10,) 140.4\n",
      "(107673, 10) 140.5\n",
      "(107673, 10) 140.5\n",
      "Iteration [21/1000], update diff norm: 0.0067, perplexity: 3285.2986\n",
      "(107673, 10) (10,) 146.7\n",
      "(107673, 10) 146.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 146.7\n",
      "(18846, 100, 10) 147.4\n",
      "(18846, 100, 10) 147.4\n",
      "(1884600, 10) 147.5\n",
      "(10,) 147.5\n",
      "(107673, 10) 147.6\n",
      "(107673, 10) 147.6\n",
      "Iteration [22/1000], update diff norm: 0.0062, perplexity: 3233.7817\n",
      "(107673, 10) (10,) 154.1\n",
      "(107673, 10) 154.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 154.1\n",
      "(18846, 100, 10) 154.8\n",
      "(18846, 100, 10) 154.8\n",
      "(1884600, 10) 154.8\n",
      "(10,) 154.8\n",
      "(107673, 10) 154.9\n",
      "(107673, 10) 154.9\n",
      "Iteration [23/1000], update diff norm: 0.0058, perplexity: 3189.3020\n",
      "(107673, 10) (10,) 161.1\n",
      "(107673, 10) 161.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 161.2\n",
      "(18846, 100, 10) 161.8\n",
      "(18846, 100, 10) 161.8\n",
      "(1884600, 10) 161.9\n",
      "(10,) 161.9\n",
      "(107673, 10) 162.0\n",
      "(107673, 10) 162.0\n",
      "Iteration [24/1000], update diff norm: 0.0054, perplexity: 3150.8950\n",
      "(107673, 10) (10,) 168.2\n",
      "(107673, 10) 168.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 168.3\n",
      "(18846, 100, 10) 168.9\n",
      "(18846, 100, 10) 168.9\n",
      "(1884600, 10) 169.0\n",
      "(10,) 169.0\n",
      "(107673, 10) 169.1\n",
      "(107673, 10) 169.1\n",
      "Iteration [25/1000], update diff norm: 0.0051, perplexity: 3117.7341\n",
      "(107673, 10) (10,) 175.1\n",
      "(107673, 10) 175.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 175.2\n",
      "(18846, 100, 10) 175.8\n",
      "(18846, 100, 10) 175.9\n",
      "(1884600, 10) 175.9\n",
      "(10,) 175.9\n",
      "(107673, 10) 176.0\n",
      "(107673, 10) 176.0\n",
      "Iteration [26/1000], update diff norm: 0.0049, perplexity: 3088.9731\n",
      "(107673, 10) (10,) 182.3\n",
      "(107673, 10) 182.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 182.3\n",
      "(18846, 100, 10) 182.9\n",
      "(18846, 100, 10) 183.0\n",
      "(1884600, 10) 183.0\n",
      "(10,) 183.0\n",
      "(107673, 10) 183.1\n",
      "(107673, 10) 183.1\n",
      "Iteration [27/1000], update diff norm: 0.0047, perplexity: 3063.8621\n",
      "(107673, 10) (10,) 189.4\n",
      "(107673, 10) 189.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 189.4\n",
      "(18846, 100, 10) 190.0\n",
      "(18846, 100, 10) 190.1\n",
      "(1884600, 10) 190.1\n",
      "(10,) 190.1\n",
      "(107673, 10) 190.2\n",
      "(107673, 10) 190.2\n",
      "Iteration [28/1000], update diff norm: 0.0045, perplexity: 3041.7588\n",
      "(107673, 10) (10,) 196.6\n",
      "(107673, 10) 196.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 196.6\n",
      "(18846, 100, 10) 197.3\n",
      "(18846, 100, 10) 197.3\n",
      "(1884600, 10) 197.3\n",
      "(10,) 197.3\n",
      "(107673, 10) 197.4\n",
      "(107673, 10) 197.4\n",
      "Iteration [29/1000], update diff norm: 0.0043, perplexity: 3022.1948\n",
      "(107673, 10) (10,) 204.3\n",
      "(107673, 10) 204.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 204.3\n",
      "(18846, 100, 10) 205.0\n",
      "(18846, 100, 10) 205.0\n",
      "(1884600, 10) 205.1\n",
      "(10,) 205.1\n",
      "(107673, 10) 205.2\n",
      "(107673, 10) 205.2\n",
      "Iteration [30/1000], update diff norm: 0.0042, perplexity: 3004.9141\n",
      "(107673, 10) (10,) 210.5\n",
      "(107673, 10) 210.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 210.6\n",
      "(18846, 100, 10) 211.3\n",
      "(18846, 100, 10) 211.3\n",
      "(1884600, 10) 211.4\n",
      "(10,) 211.4\n",
      "(107673, 10) 211.5\n",
      "(107673, 10) 211.5\n",
      "Iteration [31/1000], update diff norm: 0.0040, perplexity: 2989.8066\n",
      "(107673, 10) (10,) 215.1\n",
      "(107673, 10) 215.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 215.1\n",
      "(18846, 100, 10) 215.8\n",
      "(18846, 100, 10) 215.8\n",
      "(1884600, 10) 215.9\n",
      "(10,) 215.9\n",
      "(107673, 10) 216.0\n",
      "(107673, 10) 216.0\n",
      "Iteration [32/1000], update diff norm: 0.0039, perplexity: 2976.5181\n",
      "(107673, 10) (10,) 221.0\n",
      "(107673, 10) 221.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 221.1\n",
      "(18846, 100, 10) 221.7\n",
      "(18846, 100, 10) 221.7\n",
      "(1884600, 10) 221.8\n",
      "(10,) 221.8\n",
      "(107673, 10) 221.8\n",
      "(107673, 10) 221.8\n",
      "Iteration [33/1000], update diff norm: 0.0038, perplexity: 2964.7427\n",
      "(107673, 10) (10,) 227.2\n",
      "(107673, 10) 227.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 227.3\n",
      "(18846, 100, 10) 228.0\n",
      "(18846, 100, 10) 228.0\n",
      "(1884600, 10) 228.1\n",
      "(10,) 228.1\n",
      "(107673, 10) 228.2\n",
      "(107673, 10) 228.2\n",
      "Iteration [34/1000], update diff norm: 0.0037, perplexity: 2954.1855\n",
      "(107673, 10) (10,) 232.3\n",
      "(107673, 10) 232.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 232.3\n",
      "(18846, 100, 10) 232.9\n",
      "(18846, 100, 10) 233.0\n",
      "(1884600, 10) 233.0\n",
      "(10,) 233.0\n",
      "(107673, 10) 233.1\n",
      "(107673, 10) 233.1\n",
      "Iteration [35/1000], update diff norm: 0.0036, perplexity: 2944.6711\n",
      "(107673, 10) (10,) 237.8\n",
      "(107673, 10) 237.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 237.8\n",
      "(18846, 100, 10) 238.5\n",
      "(18846, 100, 10) 238.5\n",
      "(1884600, 10) 238.6\n",
      "(10,) 238.6\n",
      "(107673, 10) 238.7\n",
      "(107673, 10) 238.7\n",
      "Iteration [36/1000], update diff norm: 0.0035, perplexity: 2936.0596\n",
      "(107673, 10) (10,) 244.7\n",
      "(107673, 10) 244.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 244.7\n",
      "(18846, 100, 10) 245.4\n",
      "(18846, 100, 10) 245.4\n",
      "(1884600, 10) 245.5\n",
      "(10,) 245.5\n",
      "(107673, 10) 245.6\n",
      "(107673, 10) 245.6\n",
      "Iteration [37/1000], update diff norm: 0.0035, perplexity: 2928.2173\n",
      "(107673, 10) (10,) 251.2\n",
      "(107673, 10) 251.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 251.2\n",
      "(18846, 100, 10) 251.9\n",
      "(18846, 100, 10) 251.9\n",
      "(1884600, 10) 252.0\n",
      "(10,) 252.0\n",
      "(107673, 10) 252.1\n",
      "(107673, 10) 252.1\n",
      "Iteration [38/1000], update diff norm: 0.0034, perplexity: 2921.0479\n",
      "(107673, 10) (10,) 256.0\n",
      "(107673, 10) 256.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 256.1\n",
      "(18846, 100, 10) 256.7\n",
      "(18846, 100, 10) 256.8\n",
      "(1884600, 10) 256.8\n",
      "(10,) 256.8\n",
      "(107673, 10) 256.9\n",
      "(107673, 10) 256.9\n",
      "Iteration [39/1000], update diff norm: 0.0033, perplexity: 2914.4712\n",
      "(107673, 10) (10,) 261.7\n",
      "(107673, 10) 261.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 261.7\n",
      "(18846, 100, 10) 262.4\n",
      "(18846, 100, 10) 262.4\n",
      "(1884600, 10) 262.4\n",
      "(10,) 262.4\n",
      "(107673, 10) 262.5\n",
      "(107673, 10) 262.5\n",
      "Iteration [40/1000], update diff norm: 0.0033, perplexity: 2908.4045\n",
      "(107673, 10) (10,) 268.3\n",
      "(107673, 10) 268.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 268.3\n",
      "(18846, 100, 10) 268.9\n",
      "(18846, 100, 10) 269.0\n",
      "(1884600, 10) 269.0\n",
      "(10,) 269.0\n",
      "(107673, 10) 269.1\n",
      "(107673, 10) 269.1\n",
      "Iteration [41/1000], update diff norm: 0.0032, perplexity: 2902.8003\n",
      "(107673, 10) (10,) 273.7\n",
      "(107673, 10) 273.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 273.7\n",
      "(18846, 100, 10) 274.4\n",
      "(18846, 100, 10) 274.4\n",
      "(1884600, 10) 274.5\n",
      "(10,) 274.5\n",
      "(107673, 10) 274.5\n",
      "(107673, 10) 274.5\n",
      "Iteration [42/1000], update diff norm: 0.0031, perplexity: 2897.6445\n",
      "(107673, 10) (10,) 278.4\n",
      "(107673, 10) 278.5\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 278.5\n",
      "(18846, 100, 10) 279.2\n",
      "(18846, 100, 10) 279.2\n",
      "(1884600, 10) 279.3\n",
      "(10,) 279.3\n",
      "(107673, 10) 279.4\n",
      "(107673, 10) 279.4\n",
      "Iteration [43/1000], update diff norm: 0.0031, perplexity: 2892.8940\n",
      "(107673, 10) (10,) 284.1\n",
      "(107673, 10) 284.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 284.2\n",
      "(18846, 100, 10) 284.9\n",
      "(18846, 100, 10) 284.9\n",
      "(1884600, 10) 285.0\n",
      "(10,) 285.0\n",
      "(107673, 10) 285.1\n",
      "(107673, 10) 285.1\n",
      "Iteration [44/1000], update diff norm: 0.0030, perplexity: 2888.5508\n",
      "(107673, 10) (10,) 289.2\n",
      "(107673, 10) 289.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 289.2\n",
      "(18846, 100, 10) 289.9\n",
      "(18846, 100, 10) 289.9\n",
      "(1884600, 10) 290.0\n",
      "(10,) 290.0\n",
      "(107673, 10) 290.0\n",
      "(107673, 10) 290.0\n",
      "Iteration [45/1000], update diff norm: 0.0029, perplexity: 2884.5342\n",
      "(107673, 10) (10,) 295.3\n",
      "(107673, 10) 295.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 295.3\n",
      "(18846, 100, 10) 295.9\n",
      "(18846, 100, 10) 295.9\n",
      "(1884600, 10) 296.0\n",
      "(10,) 296.0\n",
      "(107673, 10) 296.1\n",
      "(107673, 10) 296.1\n",
      "Iteration [46/1000], update diff norm: 0.0029, perplexity: 2880.7722\n",
      "(107673, 10) (10,) 300.7\n",
      "(107673, 10) 300.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 300.7\n",
      "(18846, 100, 10) 301.4\n",
      "(18846, 100, 10) 301.4\n",
      "(1884600, 10) 301.4\n",
      "(10,) 301.4\n",
      "(107673, 10) 301.5\n",
      "(107673, 10) 301.5\n",
      "Iteration [47/1000], update diff norm: 0.0028, perplexity: 2877.2041\n",
      "(107673, 10) (10,) 306.1\n",
      "(107673, 10) 306.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 306.1\n",
      "(18846, 100, 10) 306.8\n",
      "(18846, 100, 10) 306.8\n",
      "(1884600, 10) 306.9\n",
      "(10,) 306.9\n",
      "(107673, 10) 307.0\n",
      "(107673, 10) 307.0\n",
      "Iteration [48/1000], update diff norm: 0.0027, perplexity: 2873.7354\n",
      "(107673, 10) (10,) 312.6\n",
      "(107673, 10) 312.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 312.6\n",
      "(18846, 100, 10) 313.3\n",
      "(18846, 100, 10) 313.3\n",
      "(1884600, 10) 313.4\n",
      "(10,) 313.4\n",
      "(107673, 10) 313.5\n",
      "(107673, 10) 313.5\n",
      "Iteration [49/1000], update diff norm: 0.0027, perplexity: 2870.4360\n",
      "(107673, 10) (10,) 317.2\n",
      "(107673, 10) 317.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 317.3\n",
      "(18846, 100, 10) 317.9\n",
      "(18846, 100, 10) 317.9\n",
      "(1884600, 10) 318.0\n",
      "(10,) 318.0\n",
      "(107673, 10) 318.1\n",
      "(107673, 10) 318.1\n",
      "Iteration [50/1000], update diff norm: 0.0026, perplexity: 2867.3525\n",
      "(107673, 10) (10,) 322.7\n",
      "(107673, 10) 322.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 322.7\n",
      "(18846, 100, 10) 323.4\n",
      "(18846, 100, 10) 323.4\n",
      "(1884600, 10) 323.4\n",
      "(10,) 323.4\n",
      "(107673, 10) 323.5\n",
      "(107673, 10) 323.5\n",
      "Iteration [51/1000], update diff norm: 0.0026, perplexity: 2864.4910\n",
      "(107673, 10) (10,) 329.0\n",
      "(107673, 10) 329.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 329.0\n",
      "(18846, 100, 10) 329.7\n",
      "(18846, 100, 10) 329.7\n",
      "(1884600, 10) 329.8\n",
      "(10,) 329.8\n",
      "(107673, 10) 329.9\n",
      "(107673, 10) 329.9\n",
      "Iteration [52/1000], update diff norm: 0.0025, perplexity: 2861.8342\n",
      "(107673, 10) (10,) 334.0\n",
      "(107673, 10) 334.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 334.0\n",
      "(18846, 100, 10) 334.6\n",
      "(18846, 100, 10) 334.7\n",
      "(1884600, 10) 334.7\n",
      "(10,) 334.7\n",
      "(107673, 10) 334.8\n",
      "(107673, 10) 334.8\n",
      "Iteration [53/1000], update diff norm: 0.0024, perplexity: 2859.3230\n",
      "(107673, 10) (10,) 340.1\n",
      "(107673, 10) 340.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 340.2\n",
      "(18846, 100, 10) 340.8\n",
      "(18846, 100, 10) 340.8\n",
      "(1884600, 10) 340.9\n",
      "(10,) 340.9\n",
      "(107673, 10) 341.0\n",
      "(107673, 10) 341.0\n",
      "Iteration [54/1000], update diff norm: 0.0024, perplexity: 2856.9775\n",
      "(107673, 10) (10,) 344.8\n",
      "(107673, 10) 344.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 344.8\n",
      "(18846, 100, 10) 345.5\n",
      "(18846, 100, 10) 345.5\n",
      "(1884600, 10) 345.6\n",
      "(10,) 345.6\n",
      "(107673, 10) 345.7\n",
      "(107673, 10) 345.7\n",
      "Iteration [55/1000], update diff norm: 0.0023, perplexity: 2854.7754\n",
      "(107673, 10) (10,) 351.1\n",
      "(107673, 10) 351.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 351.1\n",
      "(18846, 100, 10) 351.8\n",
      "(18846, 100, 10) 351.8\n",
      "(1884600, 10) 351.8\n",
      "(10,) 351.8\n",
      "(107673, 10) 351.9\n",
      "(107673, 10) 351.9\n",
      "Iteration [56/1000], update diff norm: 0.0022, perplexity: 2852.6746\n",
      "(107673, 10) (10,) 355.2\n",
      "(107673, 10) 355.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 355.3\n",
      "(18846, 100, 10) 355.9\n",
      "(18846, 100, 10) 355.9\n",
      "(1884600, 10) 356.0\n",
      "(10,) 356.0\n",
      "(107673, 10) 356.1\n",
      "(107673, 10) 356.1\n",
      "Iteration [57/1000], update diff norm: 0.0022, perplexity: 2850.6633\n",
      "(107673, 10) (10,) 361.0\n",
      "(107673, 10) 361.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 361.1\n",
      "(18846, 100, 10) 361.8\n",
      "(18846, 100, 10) 361.8\n",
      "(1884600, 10) 361.9\n",
      "(10,) 361.9\n",
      "(107673, 10) 362.0\n",
      "(107673, 10) 362.0\n",
      "Iteration [58/1000], update diff norm: 0.0021, perplexity: 2848.6782\n",
      "(107673, 10) (10,) 366.6\n",
      "(107673, 10) 366.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 366.6\n",
      "(18846, 100, 10) 367.2\n",
      "(18846, 100, 10) 367.3\n",
      "(1884600, 10) 367.3\n",
      "(10,) 367.3\n",
      "(107673, 10) 367.4\n",
      "(107673, 10) 367.4\n",
      "Iteration [59/1000], update diff norm: 0.0020, perplexity: 2846.7349\n",
      "(107673, 10) (10,) 373.2\n",
      "(107673, 10) 373.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 373.2\n",
      "(18846, 100, 10) 373.9\n",
      "(18846, 100, 10) 373.9\n",
      "(1884600, 10) 373.9\n",
      "(10,) 373.9\n",
      "(107673, 10) 374.0\n",
      "(107673, 10) 374.0\n",
      "Iteration [60/1000], update diff norm: 0.0020, perplexity: 2844.7944\n",
      "(107673, 10) (10,) 378.2\n",
      "(107673, 10) 378.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 378.2\n",
      "(18846, 100, 10) 378.9\n",
      "(18846, 100, 10) 378.9\n",
      "(1884600, 10) 378.9\n",
      "(10,) 378.9\n",
      "(107673, 10) 379.0\n",
      "(107673, 10) 379.0\n",
      "Iteration [61/1000], update diff norm: 0.0019, perplexity: 2842.8811\n",
      "(107673, 10) (10,) 383.6\n",
      "(107673, 10) 383.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 383.7\n",
      "(18846, 100, 10) 384.3\n",
      "(18846, 100, 10) 384.3\n",
      "(1884600, 10) 384.4\n",
      "(10,) 384.4\n",
      "(107673, 10) 384.5\n",
      "(107673, 10) 384.5\n",
      "Iteration [62/1000], update diff norm: 0.0019, perplexity: 2841.0151\n",
      "(107673, 10) (10,) 390.6\n",
      "(107673, 10) 390.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 390.7\n",
      "(18846, 100, 10) 391.3\n",
      "(18846, 100, 10) 391.4\n",
      "(1884600, 10) 391.4\n",
      "(10,) 391.4\n",
      "(107673, 10) 391.5\n",
      "(107673, 10) 391.5\n",
      "Iteration [63/1000], update diff norm: 0.0018, perplexity: 2839.1692\n",
      "(107673, 10) (10,) 397.0\n",
      "(107673, 10) 397.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 397.1\n",
      "(18846, 100, 10) 397.8\n",
      "(18846, 100, 10) 397.8\n",
      "(1884600, 10) 397.9\n",
      "(10,) 397.9\n",
      "(107673, 10) 397.9\n",
      "(107673, 10) 397.9\n",
      "Iteration [64/1000], update diff norm: 0.0017, perplexity: 2837.3625\n",
      "(107673, 10) (10,) 401.8\n",
      "(107673, 10) 401.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 401.9\n",
      "(18846, 100, 10) 402.5\n",
      "(18846, 100, 10) 402.5\n",
      "(1884600, 10) 402.6\n",
      "(10,) 402.6\n",
      "(107673, 10) 402.7\n",
      "(107673, 10) 402.7\n",
      "Iteration [65/1000], update diff norm: 0.0017, perplexity: 2835.5974\n",
      "(107673, 10) (10,) 407.0\n",
      "(107673, 10) 407.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 407.0\n",
      "(18846, 100, 10) 407.7\n",
      "(18846, 100, 10) 407.7\n",
      "(1884600, 10) 407.8\n",
      "(10,) 407.8\n",
      "(107673, 10) 407.8\n",
      "(107673, 10) 407.8\n",
      "Iteration [66/1000], update diff norm: 0.0016, perplexity: 2833.9336\n",
      "(107673, 10) (10,) 412.3\n",
      "(107673, 10) 412.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 412.4\n",
      "(18846, 100, 10) 413.0\n",
      "(18846, 100, 10) 413.0\n",
      "(1884600, 10) 413.1\n",
      "(10,) 413.1\n",
      "(107673, 10) 413.2\n",
      "(107673, 10) 413.2\n",
      "Iteration [67/1000], update diff norm: 0.0016, perplexity: 2832.3718\n",
      "(107673, 10) (10,) 417.7\n",
      "(107673, 10) 417.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 417.7\n",
      "(18846, 100, 10) 418.4\n",
      "(18846, 100, 10) 418.4\n",
      "(1884600, 10) 418.5\n",
      "(10,) 418.5\n",
      "(107673, 10) 418.6\n",
      "(107673, 10) 418.6\n",
      "Iteration [68/1000], update diff norm: 0.0015, perplexity: 2830.9202\n",
      "(107673, 10) (10,) 423.1\n",
      "(107673, 10) 423.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 423.1\n",
      "(18846, 100, 10) 423.8\n",
      "(18846, 100, 10) 423.8\n",
      "(1884600, 10) 423.9\n",
      "(10,) 423.9\n",
      "(107673, 10) 424.0\n",
      "(107673, 10) 424.0\n",
      "Iteration [69/1000], update diff norm: 0.0015, perplexity: 2829.5303\n",
      "(107673, 10) (10,) 429.2\n",
      "(107673, 10) 429.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 429.3\n",
      "(18846, 100, 10) 429.9\n",
      "(18846, 100, 10) 429.9\n",
      "(1884600, 10) 430.0\n",
      "(10,) 430.0\n",
      "(107673, 10) 430.1\n",
      "(107673, 10) 430.1\n",
      "Iteration [70/1000], update diff norm: 0.0014, perplexity: 2828.1611\n",
      "(107673, 10) (10,) 434.6\n",
      "(107673, 10) 434.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 434.7\n",
      "(18846, 100, 10) 435.3\n",
      "(18846, 100, 10) 435.3\n",
      "(1884600, 10) 435.4\n",
      "(10,) 435.4\n",
      "(107673, 10) 435.4\n",
      "(107673, 10) 435.4\n",
      "Iteration [71/1000], update diff norm: 0.0014, perplexity: 2826.7900\n",
      "(107673, 10) (10,) 440.3\n",
      "(107673, 10) 440.3\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 440.3\n",
      "(18846, 100, 10) 441.0\n",
      "(18846, 100, 10) 441.0\n",
      "(1884600, 10) 441.0\n",
      "(10,) 441.0\n",
      "(107673, 10) 441.1\n",
      "(107673, 10) 441.1\n",
      "Iteration [72/1000], update diff norm: 0.0014, perplexity: 2825.4707\n",
      "(107673, 10) (10,) 446.1\n",
      "(107673, 10) 446.1\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 446.1\n",
      "(18846, 100, 10) 446.8\n",
      "(18846, 100, 10) 446.8\n",
      "(1884600, 10) 446.9\n",
      "(10,) 446.9\n",
      "(107673, 10) 447.0\n",
      "(107673, 10) 447.0\n",
      "Iteration [73/1000], update diff norm: 0.0013, perplexity: 2824.2207\n",
      "(107673, 10) (10,) 451.4\n",
      "(107673, 10) 451.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 451.4\n",
      "(18846, 100, 10) 452.1\n",
      "(18846, 100, 10) 452.1\n",
      "(1884600, 10) 452.1\n",
      "(10,) 452.1\n",
      "(107673, 10) 452.2\n",
      "(107673, 10) 452.2\n",
      "Iteration [74/1000], update diff norm: 0.0013, perplexity: 2823.0291\n",
      "(107673, 10) (10,) 457.8\n",
      "(107673, 10) 457.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 457.9\n",
      "(18846, 100, 10) 458.5\n",
      "(18846, 100, 10) 458.6\n",
      "(1884600, 10) 458.6\n",
      "(10,) 458.6\n",
      "(107673, 10) 458.7\n",
      "(107673, 10) 458.7\n",
      "Iteration [75/1000], update diff norm: 0.0012, perplexity: 2821.8838\n",
      "(107673, 10) (10,) 463.6\n",
      "(107673, 10) 463.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 463.7\n",
      "(18846, 100, 10) 464.3\n",
      "(18846, 100, 10) 464.3\n",
      "(1884600, 10) 464.4\n",
      "(10,) 464.4\n",
      "(107673, 10) 464.5\n",
      "(107673, 10) 464.5\n",
      "Iteration [76/1000], update diff norm: 0.0012, perplexity: 2820.7874\n",
      "(107673, 10) (10,) 468.2\n",
      "(107673, 10) 468.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 468.2\n",
      "(18846, 100, 10) 468.9\n",
      "(18846, 100, 10) 468.9\n",
      "(1884600, 10) 468.9\n",
      "(10,) 468.9\n",
      "(107673, 10) 469.0\n",
      "(107673, 10) 469.0\n",
      "Iteration [77/1000], update diff norm: 0.0012, perplexity: 2819.6885\n",
      "(107673, 10) (10,) 474.8\n",
      "(107673, 10) 474.8\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 474.8\n",
      "(18846, 100, 10) 475.5\n",
      "(18846, 100, 10) 475.6\n",
      "(1884600, 10) 475.6\n",
      "(10,) 475.6\n",
      "(107673, 10) 475.7\n",
      "(107673, 10) 475.7\n",
      "Iteration [78/1000], update diff norm: 0.0011, perplexity: 2818.4746\n",
      "(107673, 10) (10,) 480.6\n",
      "(107673, 10) 480.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 480.7\n",
      "(18846, 100, 10) 481.4\n",
      "(18846, 100, 10) 481.4\n",
      "(1884600, 10) 481.4\n",
      "(10,) 481.4\n",
      "(107673, 10) 481.5\n",
      "(107673, 10) 481.5\n",
      "Iteration [79/1000], update diff norm: 0.0011, perplexity: 2817.1580\n",
      "(107673, 10) (10,) 485.4\n",
      "(107673, 10) 485.4\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 485.4\n",
      "(18846, 100, 10) 486.0\n",
      "(18846, 100, 10) 486.1\n",
      "(1884600, 10) 486.1\n",
      "(10,) 486.1\n",
      "(107673, 10) 486.2\n",
      "(107673, 10) 486.2\n",
      "Iteration [80/1000], update diff norm: 0.0011, perplexity: 2815.7854\n",
      "(107673, 10) (10,) 492.0\n",
      "(107673, 10) 492.0\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 492.1\n",
      "(18846, 100, 10) 492.7\n",
      "(18846, 100, 10) 492.7\n",
      "(1884600, 10) 492.7\n",
      "(10,) 492.8\n",
      "(107673, 10) 492.8\n",
      "(107673, 10) 492.8\n",
      "Iteration [81/1000], update diff norm: 0.0011, perplexity: 2814.3840\n",
      "(107673, 10) (10,) 497.2\n",
      "(107673, 10) 497.2\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 497.2\n",
      "(18846, 100, 10) 497.9\n",
      "(18846, 100, 10) 497.9\n",
      "(1884600, 10) 497.9\n",
      "(10,) 497.9\n",
      "(107673, 10) 498.0\n",
      "(107673, 10) 498.0\n",
      "Iteration [82/1000], update diff norm: 0.0010, perplexity: 2813.0974\n",
      "(107673, 10) (10,) 502.7\n",
      "(107673, 10) 502.7\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 502.7\n",
      "(18846, 100, 10) 503.4\n",
      "(18846, 100, 10) 503.4\n",
      "(1884600, 10) 503.4\n",
      "(10,) 503.4\n",
      "(107673, 10) 503.5\n",
      "(107673, 10) 503.5\n",
      "Iteration [83/1000], update diff norm: 0.0010, perplexity: 2811.9468\n",
      "(107673, 10) (10,) 508.5\n",
      "(107673, 10) 508.6\n",
      "(18846, 1, 100, 10) (1, 100, 100, 1) 508.6\n",
      "(18846, 100, 10) 509.2\n",
      "(18846, 100, 10) 509.2\n",
      "(1884600, 10) 509.3\n",
      "(10,) 509.3\n",
      "(107673, 10) 509.4\n",
      "(107673, 10) 509.4\n",
      "Iteration [84/1000], update diff norm: 0.0010, perplexity: 2810.8931\n",
      "CPU times: user 13min 6s, sys: 11min 29s, total: 24min 36s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = ContextTopicModelDebug(\n",
    "    ctx_len=500,\n",
    "    max_len=maxlen,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    n_topics=10,\n",
    ")\n",
    "model.fit(dataset.data, test_data=bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\tgun\tlaw\tgovernment\tu\tbike\tstate\tisraeli\tnew\tamerican\n",
      "\n",
      "window\tx\tuse\tsystem\tproblem\tget\tneed\tanyone\tdrive\tfile\n",
      "\n",
      "edu\twrites\tc\tarticle\tsubject\tapr\tcc\tnews\tandrew\tuiuc\n",
      "\n",
      "line\torganization\tsubject\tposting\thost\tnntp\tuniversity\tdistribution\tca\tx\n",
      "\n",
      "one\twould\tpeople\tthink\tknow\tlike\ttime\tsay\tthing\tget\n",
      "\n",
      "thanks\te\tplease\tmail\temail\thelp\tadvance\tp\tu\taddress\n",
      "\n",
      "com\twrites\tsubject\tarticle\tapr\torg\tgov\tnetcom\tnasa\taccess\n",
      "\n",
      "PAD\tryerson\tryevm\tkeith\tsola\temployer\tacps\ttmi\tteddy\tpolytechnical\n",
      "\n",
      "game\tteam\tplayer\tyear\twin\tlast\tv\tco\tplay\tpitt\n",
      "\n",
      "PAD\trainer\telin\thochreiter\teeam\tkeith\texcepted\tomission\tbye\tdonoghue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maxlen = 500, context = 10\n",
    "topk = jnp.argsort(model.phi, axis=0, descending=True)[:10, :].T  # (T, W_{top})\n",
    "reverse_vocab = {value: key for key, value in dataset.vocab.items()}\n",
    "\n",
    "for t in topk:\n",
    "    print('\\t'.join([reverse_vocab[int(idx)] for idx in t]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-modelling-attention-srLx0cG6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
