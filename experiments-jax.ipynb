{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_prepocessing import ArticlesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>From: rdell@cbnewsf.cb.att.com (richard.b.dell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>From: chriss@netcom.com (Chris Silvester)\\nSub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      From: Mamatha Devineni Ratnam <mr47+@andrew.cm...\n",
       "1      From: mblawson@midway.ecn.uoknor.edu (Matthew ...\n",
       "2      From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...\n",
       "3      From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...\n",
       "4      From: Alexander Samuel McDiarmid <am2o+@andrew...\n",
       "...                                                  ...\n",
       "18841  From: jim.zisfein@factory.com (Jim Zisfein) \\n...\n",
       "18842  From: rdell@cbnewsf.cb.att.com (richard.b.dell...\n",
       "18843  From: westes@netcom.com (Will Estes)\\nSubject:...\n",
       "18844  From: steve@hcrlgw (Steven Collins)\\nSubject: ...\n",
       "18845  From: chriss@netcom.com (Chris Silvester)\\nSub...\n",
       "\n",
       "[18846 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_20newsgroups(data_home='./data/', subset='all').data\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>TV's future down the phone line\\n\\nInternet TV...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>Cebit fever takes over Hanover\\n\\nThousands of...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
       "...                                                 ...       ...\n",
       "2197  TV's future down the phone line\\n\\nInternet TV...      tech\n",
       "2198  Cebit fever takes over Hanover\\n\\nThousands of...      tech\n",
       "2199  BT program to beat dialler scams\\n\\nBT is intr...      tech\n",
       "2200  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech\n",
       "2201  US cyber security chief resigns\\n\\nThe man mak...      tech\n",
       "\n",
       "[2202 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/bbc_text_cls.csv')\n",
    "maxlen = np.quantile(df.text.apply(len), q=0.99)\n",
    "print(len(df))\n",
    "df = df[df.text.apply(len) < maxlen].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5, 3)\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [1 1 1]\n",
      "  [2 2 2]\n",
      "  [3 3 3]\n",
      "  [4 4 4]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [2 2 2]\n",
      "  [3 3 3]\n",
      "  [4 4 4]\n",
      "  [5 5 5]]\n",
      "\n",
      " [[2 2 2]\n",
      "  [3 3 3]\n",
      "  [4 4 4]\n",
      "  [5 5 5]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[3 3 3]\n",
      "  [4 4 4]\n",
      "  [5 5 5]\n",
      "  [6 6 6]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[4 4 4]\n",
      "  [5 5 5]\n",
      "  [6 6 6]\n",
      "  [7 7 7]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[5 5 5]\n",
      "  [6 6 6]\n",
      "  [7 7 7]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "a = jnp.array([1, 2, 3, 4, 5, 6, 7], dtype=int)\n",
    "a = jnp.array(list(zip([1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7])), dtype=int)\n",
    "bounds = jnp.array([0, 2, 7], dtype=int)\n",
    "i = 2\n",
    "t = 3\n",
    "n = len(a)\n",
    "\n",
    "shifts = jnp.arange(i * 2, -1, -1)\n",
    "\n",
    "max_shift = i * 2 + n\n",
    "\n",
    "pad_token = 0\n",
    "\n",
    "result_matrix = np.full((max_shift, 2 * i + 1, t), fill_value=pad_token, dtype=int)\n",
    "\n",
    "row_indices = jnp.arange(n)[:, None] + shifts[None, :]\n",
    "col_indices = jnp.arange(2 * i + 1)\n",
    "\n",
    "result_matrix[row_indices, col_indices] = a[:, None]\n",
    "mask = jnp.all(result_matrix[:, i, :] == pad_token, axis=1)\n",
    "result_matrix = result_matrix[~mask]\n",
    "\n",
    "print(result_matrix.shape)\n",
    "print(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.144, 0.24 , 0.   , 0.24 , 0.144])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.4\n",
    "# C_i = gamma * (1 - gamma)**i\n",
    "suffix_context_weights = np.cumprod(np.full(i, (1 - gamma))) * gamma\n",
    "prefix_context_weights = suffix_context_weights[::-1]\n",
    "context_weights = np.concatenate([\n",
    "    prefix_context_weights,\n",
    "    [0.],\n",
    "    suffix_context_weights,\n",
    "])\n",
    "context_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_matrix = np.ones((n, 2 * i + 1), dtype=int)\n",
    "\n",
    "# prefix attention\n",
    "ignored_mask_prefix = np.rot90(~np.triu(np.ones(i, dtype=bool)))\n",
    "\n",
    "prefix_bounds = bounds[:-1]\n",
    "shifts = np.ones((len(prefix_bounds), i), dtype=int)\n",
    "shifts[:, 0] = prefix_bounds\n",
    "shifts = np.cumsum(shifts, axis=1)\n",
    "\n",
    "prefix_columns = np.arange(i)\n",
    "\n",
    "attn_matrix[shifts.flatten()[:, None], prefix_columns] = np.tile(ignored_mask_prefix, reps=i).T\n",
    "\n",
    "# suffix attention\n",
    "ignored_mask_suffix = np.rot90(~np.tril(np.ones(i, dtype=bool)))\n",
    "\n",
    "suffix_bounds = np.array(bounds[1:]) - i\n",
    "shifts = np.ones((len(suffix_bounds), i), dtype=int)\n",
    "shifts[:, 0] = suffix_bounds\n",
    "shifts = np.cumsum(shifts, axis=1)\n",
    "\n",
    "suffix_columns = np.arange(i + 1, i * 2 + 1)\n",
    "\n",
    "attn_matrix[shifts.flatten()[:, None], suffix_columns] = np.tile(ignored_mask_suffix, reps=i).T\n",
    "\n",
    "attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.625     , 0.375     ],\n",
       "       [0.        , 0.38461538, 0.        , 0.38461538, 0.23076923],\n",
       "       [0.1875    , 0.3125    , 0.        , 0.3125    , 0.1875    ],\n",
       "       [0.23076923, 0.38461538, 0.        , 0.38461538, 0.        ],\n",
       "       [0.375     , 0.625     , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix = context_weights * attn_matrix\n",
    "context_matrix /= context_matrix.sum(axis=1, keepdims=True)\n",
    "context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 2.        , 2.        ],\n",
       "       [1.        , 1.        , 1.        ],\n",
       "       [4.375     , 4.375     , 4.375     ],\n",
       "       [4.46153846, 4.46153846, 4.46153846],\n",
       "       [5.        , 5.        , 5.        ],\n",
       "       [5.53846154, 5.53846154, 5.53846154],\n",
       "       [5.625     , 5.625     , 5.625     ]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(context_matrix[..., None] * result_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextTopicModelDebug():\n",
    "    \"\"\"\n",
    "    Topic model which uses local context of words\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ctx_len: int,\n",
    "            vocab_size: int,\n",
    "            n_topics: int = 10,\n",
    "            gamma: float = 0.6,\n",
    "            reg_list: list = None,\n",
    "            eps: float = 1e-12,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ctx_len: one-sided context size\n",
    "            max_len: max length of a document, W_d\n",
    "            vocab_size: corpus vocabulary size, W\n",
    "            n_topics: number of topics, T\n",
    "            regularizations: list of regularizations (see `add_regularization` method)\n",
    "            eps: parameter set for balance between numerical stability and precision\n",
    "\n",
    "        Note:\n",
    "            - Total context of a word on `i`-th index is ctx_len words to the left,\\\\\n",
    "            `ctx_len` words to the right, and the word itself\n",
    "        \"\"\"\n",
    "        self.ctx_len = ctx_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_topics = n_topics\n",
    "        self._gamma = gamma\n",
    "        self._eps = eps\n",
    "\n",
    "        self._context_weights_1d = self._calc_context_weights_1d()\n",
    "\n",
    "        self._regularizations = dict()\n",
    "        if reg_list is not None:\n",
    "            for reg in reg_list:\n",
    "                self.add_regularization(reg)\n",
    "\n",
    "    def _norm(self, x: jax.Array) -> jax.Array:\n",
    "        assert jnp.any(~jnp.isnan(x)), jnp.sum(x)\n",
    "        # take x+ = max(x, 0) element-wise (perform projection on positive simplex)\n",
    "        x = jnp.maximum(x, jnp.zeros_like(x))\n",
    "        # normalize values in non-zero rows to 1\n",
    "        # (mapping from the positive simplex to the unit simplex)\n",
    "        norm = x.sum(axis=0)\n",
    "        x = jnp.where(norm > self._eps, x / norm, jnp.zeros_like(x))\n",
    "        return x\n",
    "\n",
    "    def _calc_context_weights_1d(self) -> jax.Array:\n",
    "        # C_i = gamma * (1 - gamma)**i\n",
    "        suffix_context_weights = np.cumprod(np.full(self.ctx_len, (1 - self._gamma))) * self._gamma\n",
    "        prefix_context_weights = suffix_context_weights[::-1]\n",
    "        context_weights = np.concatenate([\n",
    "            prefix_context_weights,\n",
    "            [0.],  # ignoring the word itself when calculating the context\n",
    "            suffix_context_weights,\n",
    "        ])\n",
    "        return context_weights  # (C, )\n",
    "\n",
    "    def _create_context_coeff_matrix(self, batch_size: int, attn_bounds: jax.Array) -> jax.Array:\n",
    "        attn_matrix = np.ones((batch_size, self.ctx_len * 2 + 1), dtype=bool)  # True where to attend\n",
    "\n",
    "        # prefix attention (zero out words from the previous document)\n",
    "        ignored_mask_prefix = np.rot90(~np.triu(np.ones((self.ctx_len, self.ctx_len), dtype=bool)))\n",
    "\n",
    "        prefix_bounds = attn_bounds[:-1]\n",
    "        shifts = np.ones((len(prefix_bounds), self.ctx_len), dtype=int)\n",
    "        shifts[:, 0] = prefix_bounds\n",
    "        shifts = np.cumsum(shifts, axis=1)\n",
    "\n",
    "        prefix_columns = np.arange(self.ctx_len)\n",
    "        attn_matrix[shifts.reshape(-1, 1), prefix_columns] = np.tile(ignored_mask_prefix, reps=len(prefix_bounds)).T\n",
    "\n",
    "        # suffix attention (zero out words from the next document)\n",
    "        ignored_mask_suffix = np.rot90(~np.tril(np.ones((self.ctx_len, self.ctx_len), dtype=bool)))\n",
    "\n",
    "        suffix_bounds = attn_bounds[1:] - self.ctx_len\n",
    "        shifts = np.ones((len(suffix_bounds), self.ctx_len), dtype=int)\n",
    "        shifts[:, 0] = suffix_bounds\n",
    "        shifts = np.cumsum(shifts, axis=1)\n",
    "\n",
    "        suffix_columns = np.arange(self.ctx_len + 1, self.ctx_len * 2 + 1)\n",
    "        attn_matrix[shifts.reshape(-1, 1), suffix_columns] = np.tile(ignored_mask_suffix, reps=len(suffix_bounds)).T\n",
    "\n",
    "        # calculate context weights with respect to attention and normalize weights\n",
    "        context_matrix = self._context_weights_1d * attn_matrix\n",
    "        context_matrix /= context_matrix.sum(axis=1, keepdims=True)\n",
    "        return context_matrix  # (I, C)\n",
    "\n",
    "    def _construct_context_tensor(self, data: jax.Array) -> jax.Array:\n",
    "        shifts = np.arange(self.ctx_len * 2, -1, -1)\n",
    "        max_shift = self.ctx_len * 2 + len(data)\n",
    "\n",
    "        pad_token = -10\n",
    "        shifted_matrix = np.zeros((max_shift, self.ctx_len * 2 + 1, self.n_topics))\n",
    "        shifted_matrix = np.full((max_shift, self.ctx_len * 2 + 1, self.n_topics), pad_token, dtype=float)\n",
    "\n",
    "        row_indices = np.arange(len(data))[:, None] + shifts[None, :]\n",
    "        col_indices = np.arange(self.ctx_len * 2 + 1)\n",
    "\n",
    "        shifted_matrix[row_indices, col_indices] = data[:, None]\n",
    "\n",
    "        pad_context_mask = jnp.all(jnp.isclose(shifted_matrix[:, self.ctx_len, :], pad_token), axis=1)\n",
    "        shifted_matrix = shifted_matrix[~pad_context_mask]\n",
    "        print(shifted_matrix.dtype)\n",
    "        return shifted_matrix  # (I, C, T)\n",
    "\n",
    "    def add_regularization(self, reg, tag: str = None):\n",
    "        \"\"\"\n",
    "        Add `reg` regularization to the model with `tag` identifier \\\\\n",
    "        Note:\n",
    "        - `reg` has to be a child of base `Regularization` class\n",
    "        - `tag` will use the name of the class by default\n",
    "        \"\"\"\n",
    "        if tag is None:\n",
    "            tag = reg.__name__\n",
    "        if not isinstance(reg, Regularization):\n",
    "            raise TypeError(f'Regularization [{tag}] has to be a subclass of Regularization class')\n",
    "\n",
    "        try:\n",
    "            self._regularizations[tag] = jax.grad(reg)\n",
    "        except Exception:\n",
    "            raise\n",
    "\n",
    "    def _compose_regularizations(self):\n",
    "        regs = self._regularizations.values()\n",
    "        sum_reg = lambda x: sum([1.0, ] + [reg(x) for reg in regs])\n",
    "        return jax.jit(jax.grad(sum_reg))\n",
    "\n",
    "    def fit(self, data: jax.Array, doc_bounds, max_iter: int = 1000, tol: float = 1e-3, seed: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: matrix of shape (D, W_d), containing tokenized words of each document\n",
    "            max_iter: max number of iterations\n",
    "            tol: early stopping threshold\n",
    "            seed: random seed\n",
    "        \"\"\"\n",
    "        key = jax.random.key(seed)\n",
    "        self.phi = jax.random.uniform(\n",
    "            key=key,\n",
    "            shape=(self.vocab_size, self.n_topics),\n",
    "        )  # (W, T)\n",
    "        self.n_t = jnp.full(\n",
    "            shape=(self.n_topics, ),\n",
    "            fill_value=len(data) / self.n_topics,\n",
    "        )  # (T, )\n",
    "        grad_regularization = self._compose_regularizations()\n",
    "\n",
    "        self.phi = self._norm(self.phi)\n",
    "        t_cur = time.time()\n",
    "        for it in range(max_iter):\n",
    "            # Calculate phi' (words -> topics) matrix (phi with old p_{ti})\n",
    "            print(np.array(self.phi).shape, np.array(self.n_t).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_hatch = self._norm(self.phi.T * self.n_t[:, None]).T  # (W, T)\n",
    "            print(np.array(phi_hatch).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Create theta (documents -> topics) matrix\n",
    "            phi_it_hatch = jnp.take_along_axis(phi_hatch, indices=data[:, None], axis=0)  # (I, T)\n",
    "            print(phi_it_hatch.shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_it_hatch_block = self._construct_context_tensor(phi_it_hatch)  # (I, C, T)\n",
    "            print(phi_it_hatch_block.shape, f'{time.time() - t_cur:.01f}')\n",
    "            context_matrix = self._create_context_coeff_matrix(batch_size=data.shape[0], attn_bounds=doc_bounds)\n",
    "            theta_it = np.sum(context_matrix[..., None] * phi_it_hatch_block, axis=1)  # (I, T)\n",
    "            print(np.array(theta_it).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Update p_{ti} - topic probability distribution for i-th context\n",
    "            phi_it = jnp.take_along_axis(self.phi, indices=data[:, None], axis=0)  # (I, T)\n",
    "            print(np.array(phi_it).shape, f'{time.time() - t_cur:.01f}')\n",
    "            p_ti = self._norm((phi_it * theta_it).T).T  # (I, T)\n",
    "            print(np.array(p_ti).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Update n_{t} - topic probability distribution\n",
    "            self.n_t = jnp.sum(p_ti, axis=0)  # (T, )\n",
    "            print(np.array(self.n_t).shape, f'{time.time() - t_cur:.01f}')\n",
    "\n",
    "            # Update phi (words -> topics) matrix (phi with new p_{ti})\n",
    "            indices = data.flatten()  # (I, )\n",
    "            phi_new = jnp.add.at(jnp.zeros_like(self.phi), indices, p_ti, inplace=False)  # (W, T)\n",
    "            print(np.array(phi_new).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_new += self.phi * grad_regularization(self.phi)  # (W, T)\n",
    "            print(np.array(phi_new).shape, f'{time.time() - t_cur:.01f}')\n",
    "            phi_new = self._norm(phi_new)  # (W, T)\n",
    "\n",
    "            diff_norm = jnp.linalg.norm(phi_new - self.phi)\n",
    "            print(-jnp.sum(1 * jnp.log(np.sum(theta_it * phi_it, axis=1) + self._eps)) / len(data))\n",
    "            # calculate perplexity\n",
    "            res_diff_norm = jnp.exp(-jnp.sum(1 * jnp.log(np.sum(theta_it * phi_it, axis=1) + self._eps)) / len(data))\n",
    "            print(f'Iteration [{it}/{max_iter}], update diff norm: {diff_norm:.04f}, perplexity: {res_diff_norm:.04f}')\n",
    "            self.phi = phi_new\n",
    "            if diff_norm < tol:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3414473"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ArticlesDataset(df.text.tolist())\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107671, 10) (10,) 0.0\n",
      "(107671, 10) 0.2\n",
      "(3414473, 10) 0.3\n",
      "float64\n",
      "(3414473, 21, 10) 2.4\n",
      "(3414473, 10) 4.0\n",
      "(3414473, 10) 4.0\n",
      "(3414473, 10) 4.3\n",
      "(10,) 4.3\n",
      "(107671, 10) 4.5\n",
      "(107671, 10) 4.6\n",
      "11.597601\n",
      "Iteration [0/1000], update diff norm: 0.1312, perplexity: 108836.3828\n",
      "(107671, 10) (10,) 4.8\n",
      "(107671, 10) 4.8\n",
      "(3414473, 10) 4.8\n",
      "float64\n",
      "(3414473, 21, 10) 6.6\n",
      "(3414473, 10) 8.2\n",
      "(3414473, 10) 8.2\n",
      "(3414473, 10) 8.3\n",
      "(10,) 8.3\n",
      "(107671, 10) 8.5\n",
      "(107671, 10) 8.5\n",
      "8.647864\n",
      "Iteration [1/1000], update diff norm: 0.0261, perplexity: 5697.9648\n",
      "(107671, 10) (10,) 8.5\n",
      "(107671, 10) 8.5\n",
      "(3414473, 10) 8.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 162\u001b[0m, in \u001b[0;36mContextTopicModelDebug.fit\u001b[0;34m(self, data, doc_bounds, max_iter, tol, seed)\u001b[0m\n\u001b[1;32m    160\u001b[0m phi_it_hatch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtake_along_axis(phi_hatch, indices\u001b[38;5;241m=\u001b[39mdata[:, \u001b[38;5;28;01mNone\u001b[39;00m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (I, T)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(phi_it_hatch\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt_cur\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.01f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m phi_it_hatch_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_context_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphi_it_hatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (I, C, T)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(phi_it_hatch_block\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt_cur\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.01f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m context_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_context_coeff_matrix(batch_size\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], attn_bounds\u001b[38;5;241m=\u001b[39mdoc_bounds)\n",
      "Cell \u001b[0;32mIn[3], line 107\u001b[0m, in \u001b[0;36mContextTopicModelDebug._construct_context_tensor\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    105\u001b[0m pad_context_mask \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mall(jnp\u001b[38;5;241m.\u001b[39misclose(shifted_matrix[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx_len, :], pad_token), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m shifted_matrix \u001b[38;5;241m=\u001b[39m shifted_matrix[\u001b[38;5;241m~\u001b[39mpad_context_mask]\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshifted_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shifted_matrix\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = ContextTopicModelDebug(\n",
    "    ctx_len=10,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    gamma=0.1,\n",
    "    n_topics=10,\n",
    ")\n",
    "model.fit(*dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\tgun\tlaw\tgovernment\tu\tbike\tstate\tisraeli\tnew\tamerican\n",
      "\n",
      "window\tx\tuse\tsystem\tproblem\tget\tneed\tanyone\tdrive\tfile\n",
      "\n",
      "edu\twrites\tc\tarticle\tsubject\tapr\tcc\tnews\tandrew\tuiuc\n",
      "\n",
      "line\torganization\tsubject\tposting\thost\tnntp\tuniversity\tdistribution\tca\tx\n",
      "\n",
      "one\twould\tpeople\tthink\tknow\tlike\ttime\tsay\tthing\tget\n",
      "\n",
      "thanks\te\tplease\tmail\temail\thelp\tadvance\tp\tu\taddress\n",
      "\n",
      "com\twrites\tsubject\tarticle\tapr\torg\tgov\tnetcom\tnasa\taccess\n",
      "\n",
      "PAD\tryerson\tryevm\tkeith\tsola\temployer\tacps\ttmi\tteddy\tpolytechnical\n",
      "\n",
      "game\tteam\tplayer\tyear\twin\tlast\tv\tco\tplay\tpitt\n",
      "\n",
      "PAD\trainer\telin\thochreiter\teeam\tkeith\texcepted\tomission\tbye\tdonoghue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maxlen = 500, context = 10\n",
    "topk = jnp.argsort(model.phi, axis=0, descending=True)[:10, :].T  # (T, W_{top})\n",
    "reverse_vocab = {value: key for key, value in dataset.vocab.items()}\n",
    "\n",
    "for t in topk:\n",
    "    print('\\t'.join([reverse_vocab[int(idx)] for idx in t]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-modelling-attention-srLx0cG6-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
